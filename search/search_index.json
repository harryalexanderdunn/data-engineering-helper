{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#about-me","title":"About Me","text":"<p>I am a Google Certified Principal Data Engineer with a passion for building data products and services throughout the software  development cycle. Experience of managing teams and leading collaborative data product and service designs to find the perfect cloud solution. This includes;</p> <ul> <li>Designing, building, deploying and maintaining large data transformation pipelines in Kubernetes &amp; Airflow.</li> <li>Knowledge of cloud computing patterns, workflows &amp; services and how they relate to big data platforms. </li> <li>Deploying data solutions and cloud infrastructure via CI/CD pipelines.</li> <li>Maintaining and managing Kubernetes clusters through infrastructure as code. </li> <li>Experienced in BigQuery, Python programming language and development with cloud SDK &amp; REST APIs.</li> <li>Line Management &amp; Data Engineering mentoring for large teams</li> </ul> <p></p>"},{"location":"#purpose-of-this-site","title":"Purpose of this site","text":"<p>This site contains documentation and guidance for all things data engineering with GCP and written in python. This site gives advice, guidance and code related to Data Engineering architecture and tools to help you get started and up and running quickly.</p> <p>This site includes information on:</p> <ul> <li>Python</li> <li>Markdown</li> <li>Mkdocs</li> <li>Airflow</li> <li>Dataform</li> <li>WSL2 (Linux setup)</li> <li>GCP Connectors</li> <li>Docker</li> </ul> <p>Articles</p> <p>Airflow</p> <p>Dataform</p> <p>Streamlit</p>"},{"location":"Articles/","title":"Data Engineering Articles","text":"<p>I have written a few articles on some of the topics I have been working on. In general I find it a great way to share the knowledge I have gained and also expand on what I have learnt. If I am writing about the topic I really need to fully understand the topic to ensure what I am writing is correct.</p> <p>Link to Medium Page</p>"},{"location":"Articles/#why-should-we-be-doing-this","title":"Why should we be doing this?","text":""},{"location":"Articles/#build-your-portfolio-the-teams-portfolio","title":"Build Your Portfolio &amp; the Team's Portfolio","text":"<p>One key thing about writing technical posts is that it can help build your portfolio as a developer. And it gives you an ample opportunity to be seen as skilled at what you do. Going forward we will start to showcase this to the wider team and demonstrate all the innovative work we do.</p>"},{"location":"Articles/#help-others","title":"Help Others","text":"<p>Before you got to the stage you are right now, some people helped you get there through videos and articles. That is enough reason to write contents for other people to learn from.</p>"},{"location":"Articles/#learn-more","title":"Learn More","text":"<p>The more you write on a particular thing, or let me say the more you teach a particular thing, the better you get at it. Writing technical contents may involve you writing about an application you just built that gave you a lot of stress that you don\u2019t want other developers to go through. Or you just learned a new thing and you want the world to know what you have learned.</p> <p>Doing the above mentioned, helps you build confidence in what you know.</p> <p>Source</p>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/","title":"Deploying Airflow Locally on WSL2 with Docker and Just","text":""},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#overview","title":"Overview","text":"<p>Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. It is a great orchestration tool and can be powerful addition to your tech stack. This is especially true if you use Google Cloud Composer or Astronomer. But what about running it locally? How do you test that your DAG will work before you push it to your Development or Production environment?</p> <p>I have found that the best way to run airflow locally is with Docker due to airflow needing subsequent parts to run as a whole. Docker can handle this by splitting these up into separate containers. The best two options I have found for utilising this Docker deployment are:</p> <ol> <li> <p>Dev Containers - A development container (or dev container for short) allows you to use a container as a full-featured development environment. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase, and to aid in continuous integration and testing</p> </li> <li> <p>Docker with LazyDocker &amp; Just - Docker containers managed through the use of LazyDocker to monitor the environments. These containers are spun up and spun down through the use of a Justfile which runs specific commands to setup your environments and manage your docker containers.</p> </li> </ol> <p>The Dev Container setup is definitely the easiest. However I like the control that the second method gives you as I can monitor my airflow environment and adjust settings if needed. So, this article will go through the setup to run Airflow locally using Docker with LazyDocker &amp; Just.</p>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#tech-stack","title":"Tech Stack","text":"<ul> <li>Docker: https://www.docker.com/resources/what-container/</li> <li>Just: https://github.com/casey/just</li> <li>LazyDocker: https://github.com/jesseduffield/lazydocker</li> <li>WSL2: https://learn.microsoft.com/en-us/windows/wsl/about</li> <li>Aiflow: https://airflow.apache.org/</li> <li>VSCode: https://code.visualstudio.com/</li> </ul> <p>This guide also shows how to ensure you are correctly setup with Google Cloud Platform, so you can correctly use variables and connections that have been setup in your secret manager.</p>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#setup","title":"Setup","text":""},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#installation","title":"Installation","text":"<ul> <li> <p>Ensure you have VS Code installed and python installed</p> </li> <li> <p>Ensure you have WSL installed and connected to VSCode</p> </li> <li> <p>Ensure you have python setup on WSL2: <code>sudo apt install python3 python3-pip ipython3</code></p> </li> <li> <p>You are setup with GCP SDK: run <code>gcloud auth application-default login</code> and follow the steps (you may need to add --no-browser to the end as WSL doesn't have a built in browser, also use the default browser as it can fail otherwise). This should give you a credentials file in the location: ~/.config/gcloud/application_default_credentials.json.</p> </li> <li> <p>Install just. Just is built using the Rust programming language (just git repo). There are installation instructions in the repo. We have found the most success using <code>cargo</code>. To install cargo you can use rustup to install the rust toolchain (which cargo is a part of). You will also need to run <code>sudo apt install build-essential</code> this is to ensure you have a C linker installed. Once cargo is installed (ensure you follow the last command for setup<code>. \"$HOME/.cargo/env\"</code>) you can run cargo install just.</p> </li> </ul>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#copy-the-airflow-just-code","title":"Copy the Airflow &amp; Just code","text":"<p>The key components are:</p> <ul> <li>airflow-docker-compose.yaml - contains the Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL</li> <li>airflow-docker-compose-lite.yaml - contains the Basic Airflow cluster configuration for LocalExecutor</li> <li>Dockerfile - user Docker image so that you can install libraries from your requirements.txt</li> <li>requirements.txt - python libraries for use in airflow dag </li> <li>.airflowignore - Ignore anything not ending with _dag.py</li> <li> <p>justfile - Just commands for use as a command runner</p> </li> <li> <p>Example files: https://github.com/harryalexanderdunn/local-airflow-docker/tree/main</p> </li> </ul> <p>In this setup all connections and variables will be taken from your secret manager in gcp (this can be changed for AWS). This also assumes you have setup your variables and connections with the prefix setup below:</p> <p><pre><code>{\n\"connections_prefix\": \"airflow-connections\", \n\"variables_prefix\": \"airflow-variables\"\n}\n</code></pre> You can add variables &amp; connections in the configuration as long as they have the prefix AIRFLOW_VAR or AIRFLOW_CONN.</p> <p>GCP Setup</p> <p>To ensure you are setup with GCP, you will need GOOGLE_APPLICATION_CREDENTIALS set and mounted to the container. GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/plugins/application_default_credentials.json volumes: - ~/.config/gcloud/application_default_credentials.json:/opt/airflow/plugins/application_default_credentials.json:rw If this file is not found, please follow GCP setup step in the setup.</p> <p>Volumes is important as that is what mounts your local file structure to the Docker Container. In this setup the dags are mounted from the root of your project and the other folders from the airflow folder. The .venv is also mounted to ensure there is no clash for the virtual environment.</p>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#run-the-project-setup-command-to-setup-your-libraries-and-virtual-environment-and-docker","title":"Run the project setup command to setup your libraries and virtual environment and docker","text":"<pre><code>just project-setup\n</code></pre> <p>This will install any package manager dependencies, setup your .env file, setup your .venv file, install libraries in your .venv file, install docker and install lazydocker. </p>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#run-local-airflow-to-create-a-local-airflow-instance-for-local-testing","title":"Run <code>local-airflow</code> to create a local airflow instance for local testing","text":"<p>This setup is in detached mode. So, once ready you can run lazydocker to monitor the containers.  If this is your first time installing lazydocker, you may need to restart your WSL Distro.</p> <p>  See here for more info on LazyDocker.</p> <p> The password is airflow and the user is airflow. This can be altered in the configuration. </p> <p>Large airflow instance with redis, workers, triggerer, scheduler, postgres and webserver <pre><code>just local-airflow\n</code></pre> small airflow instance with postgres and webserver <pre><code>just local-airflow-lite\n</code></pre></p> <p>To shutdown the airflow instance run <pre><code>just destroy-airflow\n</code></pre> or <pre><code>just destroy-airflow-lite\n</code></pre></p> <p></p> <p>Ensure you run the destroy command to ensure the docker container is stopped. If you do not do this the container will continue to run in your WSL environment.</p>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#the-just-file-also-has-extra-commands-for-docker-installation","title":"The just file also has extra commands for docker &amp; installation","text":"<pre><code># Docker commands\njust clean-docker-containers\njust force-remove-docker-containers\njust show-running-containers\njust show-all-containers\n\n# Install requirements.txt on Virtual Environment\njust install-libraries\njust uninstall-libraries\n</code></pre>"},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#now-you-are-ready-to-start-coding-running-and-testing-your-dags-locally-with-airflow","title":"Now you are ready to start coding, running and testing your dags locally with airflow","text":""},{"location":"Articles/Airflow%20Composer/running_airflow_locally_docker/#there-are-also-other-commands-in-the-justfile-including","title":"There are also other commands in the justfile including","text":"<p>Update the project documentation (python docs, data dictionaries and unit tests) <pre><code>just update-docs\n</code></pre></p> <p>This automates your documentation creation and before you are ready to commit your code, you can rerun this command to make sure your documentation is also up date with your changes.</p> <p>If you have any questions on the above, feel free to reach out and I'll be happy to help.</p>"},{"location":"Articles/Dataform/dataform_column_descriptions/","title":"Centralising &amp; Automating Dataform Column Descriptions","text":"<p>This is a quick guide which shows you how to centralise your column descriptions so they can be updated/edited in one place. The advantage of this is that you can avoid duplication in column descriptions and ensure the column descriptions across your dataset are identical and do not diverge. Nobody wants to go into a repo and have to edit every tables schema when a change is made.</p>"},{"location":"Articles/Dataform/dataform_column_descriptions/#how-it-works","title":"How it works","text":"<p>The backbone of dataform is javascript, the benefit of this is that they allow you to use javascript code within dataform. Here is a good article showcasing some of the features with javascript within dataform: https://blog.datatovalue.nl/dataform-javascript-utility-functions-for-ga4-tables-257dd54b034e</p> <p>The code to generate the schema descriptions is shown below:</p> <pre><code>const schemaDescriptions = {   \n    /*\n     * Column descriptions\n     */\ncolumn_one: \"Column One description, succinct and to the point\",\ncolumn_two: \"Column two description\" \n}\n\nconst getSchemaDescriptions = (customDescriptions) =&gt; {\n  return Object.assign({}, schemaDescriptions, customDescriptions);\n}\n\nmodule.exports = { getSchemaDescriptions };\n</code></pre> <p>Put this code in a constants.js file located in the includes folder in Dataform. This folder allows you to reuse code across your repository.</p> <p></p>"},{"location":"Articles/Dataform/dataform_column_descriptions/#applying-the-schema-code","title":"Applying the schema code","text":"<p>Once you have this javascript code you can implement it in all your SQLX scripts. Apply the function to columns in the config to ensure the descriptions are added to the table.</p> <pre><code>config {\n  type: \"table\",\n  columns: constants.getSchemaDescriptions()\n}\n\nSELECT\n  column_one\n  column_two\nFROM ${ref(\"table\")}\n</code></pre> <p>When you have run your dataform pipeline your table will be created with the column descriptions added as shown below.</p> <p></p> <p>Its as simple as that! No more excuses for missing column descriptions.</p>"},{"location":"Articles/Documentation%20Series/automating_data_dictionaries/","title":"Automating Data Dictionary creation and maintenance","text":"<p>Nobody wants to spend all their time writing documentation, so lets make some code to do it for you. I have written a script which takes tables from any bigquery dataset (that you have access to) and generates a data dictionary for each table in the dataset within a single md file. If you display this in a centralised site such as Mkdocs you can then split your data dictionaries by dataset and they are all easily searchable through the search bar on Mkdocs. This makes looking at data very powerful as its easy to search for certain key words over many different projects.</p> <p>Please see my previous article for more information on setting up a centralised documentation site.</p>"},{"location":"Articles/Documentation%20Series/automating_data_dictionaries/#how-it-works","title":"How it works","text":"<p>This is specifically for extracting data information out of bigquery and displaying it as a data dictionary within a central documentation site.</p> <ol> <li>You need to ensure your columns descriptions are labelled appropriately. See here for information on how to do this within dataform: Centralising &amp; Automating Dataform Column Descriptions.</li> <li>You need to have access to the appropriate tables and access to the metadata tables: \"INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\"</li> <li>If your table schemas or descriptions change you will need to rerun this code and push to the repo.</li> <li>Have pandas &amp; google-cloud-bigquery installed to use the libraries</li> </ol>"},{"location":"Articles/Documentation%20Series/automating_data_dictionaries/#the-code","title":"The Code","text":"<pre><code>from google.cloud import bigquery\nimport pandas as pd\n\ndef bq_to_df(project, dataset, table, columns):\n    \"\"\"\n    Extracts table from BigQuery and returns a dataframe of the table\n    Args:\n        project (str): project location of table\n        dataset (str): dataset location of table\n        table (str): table name of table\n        columns (str): columns to be extracted from table (split by comma)\n\n    Returns:\n        df (dataframe): dataframe of data within table for specified columns\n    \"\"\"\n    client = bigquery.Client()\n    sql = f\"SELECT {columns} FROM `{project}.{dataset}.{table}`\"\n    df = client.query(sql).to_dataframe()\n    return df\n\ndef create_md_file_from_string(string, filename):\n    \"\"\" \n    creates or opens a file based on a file name and writes/overwrites the data within the file with from the string input\n    Args:\n        string (str): string to be written into the file\n        filename (str): name and location of file to be written to\n    \"\"\"\n    with open(filename, \"w\") as file:\n        file.write(string)\n\ndef extract_information_schema_to_markdown(project, dataset):\n    \"\"\"\n    Extract the information schema from BQ and translate to markdown\n    Args:\n        project (str): project location of table\n        dataset (str): dataset location of table\n    \"\"\"\n    columns = \"table_name, column_name, data_type, description, collation_name, rounding_mode\"\n    table = \"INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\"\n\n    df = bq_to_df(project, dataset, table, columns)\n    tables = df[\"table_name\"].unique()\n    md_string = f\"# {dataset} \\n \\n## Data Dictionary for {dataset} in {project} \\n\"\n    for t in tables:\n        filtered_df = df.loc[df[\"table_name\"] == t]\n        filtered_df = filtered_df.reset_index(drop=True)\n        table_string = filtered_df.to_markdown()\n        md_string = md_string + \"\\n\" + f\"### {t}\" + \"\\n\" + table_string + \"\\n\"\n    create_md_file_from_string(md_string, f\"docs/data_dictionary/{dataset}.md\")\n</code></pre> <p>The key function is extract_information_schema_to_markdown which is highlighted above. This creates the data dictionary. To Run the code you can set the datasets you want to extract in \"if name == \"main\":\". Which can be run through your IDE or through <code>python3 &lt;path&gt;</code>. Example is shown below.</p> <pre><code>if __name__ == \"__main__\":\n    extract_information_schema_to_markdown(\"gcp-project\", \"gcp_dataset\")\n    extract_information_schema_to_markdown(\"gcp-project\", \"gcp_dataset_RAW\")\n    extract_information_schema_to_markdown(\"gcp-project\", \"gcp_dataset_ACCESS\")\n</code></pre> <p>Once you have run this code the files will exist in the docs folder in your repo. You will need to then add, commit and push them into your repo.</p> <pre><code>git add .\ngit commit -am\"added data dictionaries for ...\"\ngit push\n</code></pre> <p>\ud83d\ude80 Now you are all done. Automated data dictionaries in minutes. If you make any changes to the schemas or add new tables, just rerun this code to overwrite the data dictionaries with the new information. These data dictionaries can now be viewed within your software development platform (e.g Gitlab, Github) or Centralised site (e.g Mkdocs).</p> <p>To improve upon this we can add more columns to the data dictionary with other tables available within the INFORMATION_SCHEMA. Next steps would also include adding this into the CI/CD process to ensure the schema is always updated when changes are made.</p> <pre><code>SELECT\n  COLUMN_FIELD_PATHS.table_catalog,\n  COLUMN_FIELD_PATHS.table_schema,\n  COLUMN_FIELD_PATHS.table_name, \n  COLUMN_FIELD_PATHS.column_name, \n  COLUMN_FIELD_PATHS.data_type, \n  COLUMN_FIELD_PATHS.description, \n  COLUMN_FIELD_PATHS.collation_name, \n  COLUMN_FIELD_PATHS.rounding_mode,\n  COL.is_nullable,\n  COL.is_partitioning_column,\n  COL.clustering_ordinal_position, \n  ordinal_position\nFROM `gcp-project.gcp_dataset.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS` COLUMN_FIELD_PATHS\nLEFT JOIN `gcp-project.gcp_dataset.INFORMATION_SCHEMA.COLUMNS` COL\n  ON COLUMN_FIELD_PATHS.table_catalog = COL.table_catalog\n  AND COLUMN_FIELD_PATHS.table_schema = COL.table_schema\n  AND COLUMN_FIELD_PATHS.table_name = COL.table_name\n  AND COLUMN_FIELD_PATHS.column_name = COL.column_name\nORDER BY table_name ASC, ordinal_position ASC\n</code></pre> <p>This is only the beginning</p> <p>This is only a starting point and more can be done to make this code simple and efficient. So I would like to hear your thoughts!! I would love to hear about any improvements you would make to this or additions so everyone can make writing data dictionaries easy and super fast. Most importantly I  want this information to be readable and accessable, so people can find data quickly and easily.</p>"},{"location":"Articles/Documentation%20Series/automating_documentation_creation/","title":"Automating Project Repository setup &amp; documentation creation with a Justfile","text":""},{"location":"Articles/Documentation%20Series/automating_documentation_creation/#overview","title":"Overview","text":"<p><code>just</code> is a handy way to save and run project-specific commands. More information on just is located here: https://github.com/casey/just/tree/master</p> <p>Commands, called recipes, are stored in a file called <code>justfile</code> with syntax inspired by <code>make</code></p> <p></p> <p>You can then run them with <code>just RECIPE</code>:</p> <pre><code>$ just test-all\ncc *.c -o main\n./test --all\nYay, all your tests passed!\n</code></pre> <p><code>just</code> has a ton of useful features, and many improvements over <code>make</code> which you can read about in the README document within just itself (https://github.com/casey/just/tree/master).</p> <p>Just will be useful for overall setup of a project running specific commands and automating tasks. This would include:</p> <ul> <li>Virtual environment creation</li> <li>Libraries installation</li> <li>Local Environment variables setup (.env file) from secret manager</li> <li>Airflow instance setup through docker</li> <li>Repository Checks i.e summaries, linting etc</li> <li>Documentation automation</li> </ul> <p>Specifically we are using Just here to run our virtual environment setup and documentation files. This will allow users to quickly get started on a virtual environment and use it within WSL2 and also ensure their documentation is up to date as their codebase progresses.</p> <p>This is a work in progress, so as we add more automation we can add more to this justfile</p>"},{"location":"Articles/Documentation%20Series/automating_documentation_creation/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have Just available to run in WSL2. The best was to install just is to use cargo through rust.</p> <ol> <li>To install rust: <code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</code></li> <li>to install just: <code>cargo install just</code></li> </ol> <p>For documentation automation we will be using lazydocs and the GCP data dictionaries automation code.</p> <ol> <li>Ensure you have <code>lazydocs</code> within your requirements.txt</li> <li>Ensure you have a docs folder with the data dictionaries automation script added, article info </li> </ol> <pre><code>docs\n  |-- assets\n  |-- automation\n        |-- data_dictionary.py\n</code></pre>"},{"location":"Articles/Documentation%20Series/automating_documentation_creation/#setup","title":"Setup","text":"<p>Now you can start to create your justfile. Ensure the python version matches your repo and the folder structure aligns to the below for documentation.</p> <p>THIS IS CURRENTLY BEING REWRITTEN</p> <p>I am planning to add more to this area, so you should see more to follow. I am also not precious about the justfile vs makefile debate, so if you do have any thoughts send me a message. It would be great to get some outside opionion/thoughts on this and ideas for next steps.</p>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/","title":"Documentation as Code: Centralising documentation across multiple repositories (Methodology)","text":""},{"location":"Articles/Documentation%20Series/central_docs_methodology/#documentation-as-code-philosophy","title":"Documentation as Code Philosophy","text":"<p>The generic goal of good documentation is to make finding information as easy as possible for either your user or developer. It should be clear, concise, avoid repetition and be up to date. Even though documentation is so important, as developers we know writing documentation can be boring and keeping it up to date can be a tedious process. This is where documentation as code becomes a crucial component to a developers arsenal. There are many articles on the internet explaining the benefits of documentation as code, so this will not be another one of them. This article is to describe a methodology to create and maintain a centralised documentation as code repository for your team.</p>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/#problem-statement","title":"Problem Statement","text":"<p>The Philosophy for documentation as code is simple, \"documentation is created and managed using the same tools/workflows as your software development team\". This creates an efficient, structured and organised methodical approach to documentation. It works really well for singular projects or small teams to create complete documentation.</p> <p>A big issue starts to show when teams get bigger, repositories grow &amp; more and more repositories and folders are created across your software development platform. How do you deploy and maintain this documentation codebase?</p> <p>Note</p> <p>I want to be able to allow teams to write documentation as code within their repositories but not have to worry about deployment or management of a documentation site.</p>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/#the-solution","title":"The Solution","text":"<p>There are many solutions you could use to solve this problem. These solution generally come under two headings;</p> <ol> <li>Create a centralised documentation repository, containing just documentation for all other repositories</li> <li>Create a centralised repository that extracts the documentation from the other repositories</li> </ol> <p>Solution 1 although simplistic does not work for me as I want to keep my documentation within the repository it is documenting. This is because it would allow for documentation automation within the repository (An article for another day). This meant a centralised repository was needed and furthermore it needed to connect to any child repositories. A child repository in this scenario is a separate repository to the centralised repository, it can be any repository in any location on the software development platform. This child repository will have its documentation linked to a deployed centralised site.</p>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/#tech-stack","title":"Tech Stack","text":"<p>To Achieve this I need to choose a tech stack. There is a plethora of choices out there and certain factors may push you towards a different route. In this case I am working within Google Cloud Platform and I am using Gitlab as the software development platform.</p> <ul> <li>Markdown: Documentation will be written, formatted and displayed using markdown.</li> <li>MkDocs Material: Documentation will be created using MkDocs. Specifically MkDocs Material was chosen because of its variety of plugins and extras to give extended capabilities for the user.</li> <li>GitLab: Software development platform where all of our code is stored. (This also works on Github).</li> <li>App Engine:  Fully managed, serverless platform for developing and hosting web applications at scale.</li> </ul> <p>The main reason for choosing MkDocs Material is it's Simplicity. MkDocs Material has a variety of plugins available which make the tool very powerful. One of which is the mono repo plugin, which allows you to connect the documentation of multiple repositories in one convenient location using submodules. This means that a team can decide whether to host their own documentation site using MkDocs or use the central site which will be hosted for them. The diagram below shows how the process would flow from a child repository to centralised repository.</p> <p>There is also another plugin called multi repo plugin. Unlike the mono repo plugin it does not use submodules. This means you can still Keep your documentation close to your code and it be findable on one site. But, with this implementation submodules do not need to be downloaded or updated on the central repository. I am currently looking at implementing this plugin within gitlab. However, I am experiencing bugs with the gitlab integration. I will write up an update when those bugs are fixed. The Github integration is currently bug free, making it an excellent choice if you are using Github.</p> <pre><code>graph LR;\n  A[Project Repo #main] --&gt; C[Documentation Check];\n  C --&gt;|Pass| D(Trigger Central Documentation Repo CI/CD);\n  C --&gt;|Fail| B(Alert Project Tech Lead);\n  D --&gt; E[Central Documentation Check]\n  H[Central Documentation Repo #main] --&gt; E\n  E --&gt;|Pass| F[Central Documentation App];\n  E --&gt;|Fail| G(Alert BAU Tech Lead);</code></pre>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/#overview-of-mkdocs-material","title":"Overview of MkDocs Material","text":"<p>It's very simple to get started with MkDocs Material, they have a great guide located here.</p> <p>I recommend using MkDocs Material over its competitors as it has a substantial amount of plugin extensions as well as a sponsorware release strategy. You have the choice to pay for premium features through it's sponsor service or you can use the open source version. As more people/companies subscribe to the subscription service more features are released to the open source version. I like this model as it generates stability and if you want further features you can pay a small fee to join the \"insiders\" community.</p> <p>As mentioned above MkDocs was chosen because of its mono repo plugin &amp; multi repo plugin. I have also listed some other MkDocs plugins below that I have found to be useful:</p> <ol> <li>mkdocs-mermaid2</li> <li>neo</li> <li>mkdocstrings</li> <li>mkdocs_macros_plugin</li> <li>mkdocs-pdf-export-plugin</li> </ol>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/#deploying-your-documentation-site","title":"Deploying your Documentation Site","text":"<p>Once you have a central site running locally you will not want to serve this site forever from your laptop/computer, so you will need to host this site somewhere. There are some great choices for website hosting out there and as this is a static site the options will also be cheap. I won't go in to details of the options as I chosen Google Cloud Platform - App Engine. I have chosen to host it with App Engine as it fits seamlessly into my tech stack with the covered service providing a Monthly Uptime Percentage to the customer of at least 99.95%. The static sites on App Engine do not need any compute power so the costs are also minimal. This gives me stability and cost effectiveness for my site. The added bonus of App Engine is that is integrated with Google's Identity Aware Proxy which is used to guard access to the application.</p> <p>To deploy this app I will be using a CI/CD pipeline within gitlab. Referring to our diagram at the beginning of the article I want to be able to push changes from my child repository into my central repository. This is done indirectly by triggering the central repository through a push to the main branch on a child repository. Another option is to set a schedule on the central repository CI/CD pipeline to update periodically i.e every hour/day (I would recommend this option if your central repository becomes very large). Once deployed this provides me with a central site that displays up to date documentation that is easy to create, manage and update. Documentation doesn't have to be an afterthought or a chore anymore.</p>"},{"location":"Articles/Documentation%20Series/central_docs_methodology/#technical-implementation","title":"Technical implementation","text":"<p>I have written an article on the technical implementation of this solution which covers,</p> <ul> <li>MkDocs Material setup</li> <li>MkDocs Monorepo setup</li> <li>Submodule setup</li> <li>CI/CD setup in Gitlab</li> <li>App Engine deployment within Gitlab</li> <li>Identity-Aware Proxy setup</li> </ul> <p>For this technical article please follow this link</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/","title":"Documentation as Code: Centralising documentation across multiple repositories (Technical Implementation)","text":"<p>This article walks you through the technical implementation of creating &amp; deploying a centralised documentation site linking multiple repositories across your software development platform. Please Read this article if you want a deeper understanding of what I am creating in this technical walkthrough.</p> <p></p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#problem-statement","title":"Problem Statement","text":"<p>The Goal</p> <p>I want to be able to allow teams to write documentation as code within their repositories but not have to worry about deployment or management of a documentation site. In this walkthrough we will refer to these repositories as child repositories.</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#the-solution","title":"The Solution","text":"<p>The Solution</p> <p>Create a centralised repository that extracts the documentation from the other repositories using the tech stack listed below. This centralised repository will deploy a centralised site hosting this documentation for users and developers to access.</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#tech-stack","title":"Tech Stack","text":"<ul> <li>Markdown: Documentation will be written, formatted and displayed using markdown.</li> <li>MkDocs Material: Documentation will be created using MkDocs. Specifically MkDocs Material was chosen because of its variety of plugins and extras to give extended capabilities for the user.</li> <li>GitLab: Software development platform where all of our code is stored. (This also works on Github).</li> <li>mono repo plugin: MkDocs plugin to connect the documentation of multiple repos in one convenient location using submodules.</li> <li>App Engine: Fully managed, serverless platform for developing and hosting web applications at scale.</li> <li>Identity-Aware Proxy: IAP lets you establish a central authorization layer for applications accessed by HTTPS, so you can use an application-level access control model instead of relying on network-level firewalls.</li> </ul> <p>There is also another plugin called multi repo plugin. Unlike the mono repo plugin it does not use submodules. This means you can still Keep your documentation close to your code and it be findable on one site. But, with this implementation submodules do not need to be downloaded or updated on the central repository. I am currently looking at implementing this plugin within gitlab. However, I am experiencing bugs with the gitlab integration. I will write up an update when those bugs are fixed. The Github integration is currently bug free, making it an excellent choice if you are using Github.</p> <pre><code>graph LR;\n  A[Project Repo #main] --&gt; C[Documentation Check];\n  C --&gt;|Pass| D(Trigger Central Documentation Repo CI/CD);\n  C --&gt;|Fail| B(Alert Project Tech Lead);\n  D --&gt; E[Central Documentation Check]\n  H[Central Documentation Repo #main] --&gt; E\n  E --&gt;|Pass| F[Central Documentation App];\n  E --&gt;|Fail| G(Alert BAU Tech Lead);</code></pre>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#creating-a-documentation-site-using-mkdocs","title":"Creating a Documentation Site using MkDocs","text":"<p>It's very simple to get started with MkDocs, they have a great guide located here.</p> <p>I'll outline the key points below:</p> <p>Step 1: Create a Repository with the structure below, where index.md is your home page:</p> <pre><code>\u251c\u2500\u2500\u2500 docs\n|    \u2514\u2500\u2500\u2500 index.md\n\u251c\u2500\u2500\u2500 overrides\n\u251c\u2500\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500\u2500 requirements.txt   \n</code></pre> <p>Ensure all your documentation is located within the docs folder. The overrides folder can be used to override settings set by MkDocs. More information about this can be found on the MkDocs Material website (This folder will not be explored in this article). The requirements.txt file should contain any installed libraries. I would recommend using specific versions tailored to your python version as this aligns with best practices. The below is for python 3.9.</p> <pre><code>mkdocs==1.4.3\nmkdocs-material==9.1.3\nmarkdown==3.3.7\nmkdocs-monorepo-plugin\n</code></pre> <p>below are the basic settings I recommend for starting out on MkDocs Material. These settings give you a good starting point for using markdown to its full potential. Importantly it lets you use mono repo to connect child repositories to the central repository. I would recommend researching the extensions so you can edit the design of the site yourself.</p> <pre><code>site_name: Central Documentation Site\nsite_description: 'A template for future MkDocs wikis'\nsite_author: 'tbc'\n\ncopyright: |\n  &amp;copy; Author\n\nnav:\n  - Home: \n    - index: 'index.md'\n\nplugins:\n  - search:\n      separator: '[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&amp;[lg]t;|(?!\\b)(?=[A-Z][a-z])'\n  - monorepo\n\ntheme:\n  name: material\n  custom_dir: overrides\n  features:\n    - announce.dismiss\n    - content.code.annotate\n    - content.code.copy\n    - content.tabs.link\n    - content.tooltips\n    - navigation.footer\n    - navigation.indexes\n    # - navigation.sections\n    - navigation.tabs\n    - navigation.tabs.sticky\n    - navigation.top\n    - search.highlight\n    - search.share\n    - search.suggest\n    # - toc.follow\n  language: en\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    - scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n  shortcuts:\n    help: 191 # ?\n    next: 78 # n\n    previous: 80 # p\n    search: 83 # s\n  font:\n    text: Roboto\n    code: Roboto Mono\n  icon:\n    logo: 'material/library'\n    repo: 'material/library'\n    admonition:\n      note: octicons/tag-16\n      abstract: octicons/checklist-16\n      info: octicons/info-16\n      tip: octicons/flame-16\n      success: octicons/check-16\n      question: octicons/question-16\n      warning: octicons/alert-16\n      failure: octicons/x-circle-16\n      danger: octicons/zap-16\n      bug: octicons/bug-16\n      example: octicons/beaker-16\n      quote: octicons/quote-16\n      data: octicons/database-16\n      squirell: octicons/squirrel-16\n      goal: octicons/goal-16\n      rocket: octicons/rocket-16\n\nmarkdown_extensions:\n  - abbr\n  - admonition\n  - attr_list\n  - def_list\n  - footnotes\n  - md_in_html\n  - markdown_inline_graphviz\n  - mdx_truly_sane_lists\n  - plantuml_markdown\n  - tables\n  - pymdownx.critic\n  - pymdownx.caret\n  - pymdownx.details\n  - pymdownx.extra\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.mark\n  - pymdownx.magiclink\n  - pymdownx.smartsymbols\n  - pymdownx.snippets\n  - pymdownx.tilde\n  - toc:\n      permalink: true\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n      options:\n        custom_icons:\n          - overrides/.icons\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.tabbed:\n      alternate_style: true\n  - pymdownx.tasklist:\n      custom_checkbox: true\n</code></pre> <p>Step 2: Create a virtual environment and Install MkDocs Material, this can be done using pip (ideally) in a virtual environment. To install Material for MkDocs open up a terminal and run the command at your root level:</p> <pre><code>workon &lt;environment_name&gt;\n</code></pre> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Step 3: MkDocs has two options for creating the site, <code>serve</code> and <code>build</code>. <code>mkdocs build</code> will build your site in a directory called \"site\" at your root. MkDocs also comes with a built-in dev-server that lets you preview your documentation as you work on it. This is created by running the serve command at your root level. This will serve your site on <code>http://localhost:8000/</code>.</p> <pre><code>mkdocs serve\n</code></pre> <p></p> <p>Step 4: Open up <code>http://localhost:8000/</code> to see your site locally. You now have a central documentation site using MkDocs, this is where you can create centralised documentation for your team. You should see a page similar to the screenshot below.</p> <p></p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#implementing-mkdocs-monorepo-plugin","title":"Implementing MkDocs Monorepo Plugin","text":"<p>We now have a central documentation repository, where documentation can be added and organised into directories within the repo. The monorepo plugin has been installed and is now ready to use, so now we can connect our child repositories. The monorepo plugin works by using git submodules. This means that the central repository brings in a child repository's documentation using git submodules.</p> <p>The submodule approach means that for every repository you wish to connect to the site, it first needs to be added using git submodules</p> <p>Step 1: Create directories for the specific child repository locations in your central repository.</p> <pre><code>\u251c\u2500\u2500\u2500 docs\n|    \u251c\u2500\u2500\u2500 top_secret_repos\n|    \u2514\u2500\u2500\u2500 index.md\n\u251c\u2500\u2500\u2500 overrides\n\u251c\u2500\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500\u2500 requirements.txt \n</code></pre> <p>Step 2: Create or use an already built child repository and ensure your documentation within this repository is written in markdown, is legible and is located within a docs folder. You can add an index page in this docs folder to also have a front page for your site. This will also be viewable in Gitlab when you open your repository in Gitlab and look in the docs folder. This index page can be different or identical to your readme page at root level in your repo. In this scenario I have added the markdown files in the structure below.</p> <pre><code>\u2514\u2500\u2500\u2500 docs\n    \u251c\u2500\u2500\u2500 index.md\n    \u2514\u2500\u2500\u2500 secret_logic.md\n</code></pre> <p>Step 3: Create an mkdocs.yml file at the root level in your repo. This yaml file must contain a site_name (project directory/repo_name) a docs_dir (./docs) and a nav. below is a example of what the yaml should look like. This yaml defines where your documentation will be structured in the site. You can move the files around in the nav section for how you want your documentation to be shown. If a md page is not listed in the nav section, it will not appear on the site. Ideally your site name should follow the path of your submodule in your central repository</p> <pre><code>site_name: top_secret_repos/top-secret-repo-one\n\ndocs_dir: ./docs\n\nnav:\n- Home: 'index.md'\n- More Docs: \n    - Logic: 'top_secret/secret_logic.md'\n</code></pre> <p>Step 4: Add your child repository as a submodule to your central repository by running the git submodule command below.</p> <pre><code>git submodule add git@\"gitlab_domain\":top-secret-repo-one.git docs/top_secret_repos/top-secret-repo-one\n</code></pre> <p>By adding \"docs/top_secret_repos/top-secret-repo-one\" at the end of the command it specifies where you would like to store your submodule within the directories of the central repository.</p> <p>For the submodule to link properly it has to use a relative path which is linked by editing .gitmodules. If your central repository and you child repository are both at the root level of your gitlab instance you would follow the url pattern below. The url follows the relative position of the child repo in relation to the central repository. i.e if the child repo was in a project called \"Projects\" the url would be <code>url = ../Projects/top-secret-repo-one.git</code>.</p> <pre><code>[submodule \"docs/top_secret_repos/top-secret-repo-one\"]\n    path = docs/top_secret_repos/top-secret-repo-one\n    url = ../top-secret-repo-one.git\n</code></pre> <p>Run the command below to ensure your central repository has the latest up to date submodule:</p> <pre><code>git submodule update --remote --merge\n</code></pre> <p>The repository structure will now look like this:</p> <pre><code>\u251c\u2500\u2500\u2500 docs\n|    \u251c\u2500\u2500\u2500 top_secret_repos\n|    |    \u2514\u2500\u2500\u2500 top-secret-repo-one\n|    \u2514\u2500\u2500\u2500 index.md\n\u251c\u2500\u2500\u2500 overrides\n\u251c\u2500\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500\u2500 requirements.txt \n</code></pre> <p>Step 5: You can now add the submodule to your nav section in the central repositories mkdocs.yml. <code>yaml '!include ./docs/project-repo/mkdocs.yml'</code>. This tells mkdocs to bring through the mkdocs.yml file within your submodule and attach the information to the nav section.</p> <pre><code>nav:\n  - Home: \n    - index: 'index.md'\n  - Top Secret Repo 1: '!include ./docs/top_secret_repos/top-secret-repo-one/mkdocs.yml'\n</code></pre> <p>And that's it! \ud83d\ude80 Your project is now connected to the central repository and your child repository's documentation can be rendered on a centralised documentation site. To test this run <code>mkdocs serve</code> to bring through your child repositories documentation into the central site. Open up <code>http://localhost:8000/</code> to view this documentation.</p> <p></p> <p>If you want to add more child repositories to the central repository just repeat the steps above for your other child repositories and your central documentation site will start to grow. If anyone wants to add a new repository to the central site this can be achieved with a merge request.</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#deploying-your-documentation-site","title":"Deploying your Documentation Site","text":"<p>You now have a central site running locally. But you cannot serve this site forever from your laptop/computer, you need to host this site somewhere. I have chosen to host the site using App Engine as it fits seamlessly into my tech stack with the covered service providing a Monthly Uptime Percentage to the customer of at least 99.95%. The static sites on App Engine do not need any compute power so the costs are also minimal. This gives me stability and cost effectiveness for my site. The added bonus of App Engine is that is integrated with Google's Identity-Aware Proxy which is used to guard access to the application.</p> <p>To deploy this app I will be using a CI/CD pipeline within gitlab. Referring to our diagram at the beginning of the article we want to be able to push changes from our child repository into our central repository. This is done indirectly by triggering the central repository through a push to the main branch on a child repository. Another option is to set a schedule on the central repository CI/CD pipeline to update periodically i.e every hour/day (I would recommend this option if your central repository becomes very large).</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#app-engine-setup-with-identity-aware-proxy-iap","title":"App Engine Setup with Identity-Aware Proxy (IAP)","text":"<p>App Engine requirements:</p> <ul> <li>Ensure the App Engine API is enabled</li> <li>Ensure the Identity-Aware Proxy (IAP) API is enabled</li> <li>Ensure your Gitlab service account has the App Engine Deployer permission in IAM</li> <li>App Engine Admin is useful for utilising App Engine to its full potential</li> <li>IAP Admin is useful for implementing access control</li> </ul> <p>Once the App Engine API is enabled, click create an application to get started. Configure the application to get setup, choose the location and service account for your default application. The first app created on App Engine has to be called default.</p> <p></p> <p></p> <p>Once the IAP API is enabled go to the main page where you may see a red banner. This will appear if your project does not have an OAuth consent screen. To do this click on the \"configure consent screen\" button. Your consent screen will apply to your whole project and you have the choice to make it internal or external to your organisation. The key fields to fill out are:</p> <ul> <li>App name</li> <li>User Support Email</li> <li>Developer contact information</li> </ul> <p></p> <p>The other fields can be added or left blank. It will then ask you for oauth scopes, none are needed for this application so you can save and continue without filling this in. Once applied it will show you a summary of your consent screen. This screen will appear if users need to sign in or have been restricted access to the application.</p> <p>If you need any extra information look at the google guide on this topic: https://cloud.google.com/beyondcorp-enterprise/docs/securing-app-engine</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#gitlab-cicd-setup","title":"Gitlab CI/CD Setup","text":"<p>Step 1: In Gitlab you need to ensure that the child repository has the ability to trigger the central repository. Otherwise you cannot trigger the central CI/CD pipeline from your child repository and so the central repository cannot get the docs from your child repository. To do this you must add the central repository to the Token Access section in your child repository's CI/CD settings. To do this open CI/CD settings and click add project.</p> <p></p> <p>Step 2: Add files to your central repository.</p> <p>To create a CI/CD pipeline within the central repository you need to add a .gitlab-ci.yml file. You also need an app.yaml file to specify the settings/setup for your app.</p> <pre><code>\u251c\u2500\u2500\u2500 docs\n|    \u251c\u2500\u2500\u2500 top_secret_repos\n|    |    \u2514\u2500\u2500\u2500 top-secret-repo-one\n|    \u2514\u2500\u2500\u2500 index.md\n\u251c\u2500\u2500\u2500 .gitlab-ci.yml\n\u251c\u2500\u2500\u2500 app.yaml\n\u251c\u2500\u2500\u2500 overrides\n\u251c\u2500\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500\u2500 requirements.txt \n</code></pre> <p>The app.yaml file follows a standard template from google. The first app you ever make must be called default, after that the service name can be uniquely named. The yaml file is setup to use the site directory created from mkdocs build in the central repository.</p> <pre><code>runtime: python39\nservice: default\n\nhandlers:\n  # static files with a URL ending with a file extension\n  # (e.g. favicon.ico, manifest.json, jylade.png)\n  - url: /(.*\\..+)$\n    static_files: site/\\1\n    upload: site/(.*\\..+)$\n\n  # index page\n  - url: /\n    static_files: site/index.html\n    upload: site/index.html\n\n  # anything that ends with a slash (e.g. /docs/)\n  - url: /(.*)/$\n    static_files: site/\\1/index.html\n    upload: site/(.*)\n\n  # anything else (e.g. /docs)\n  - url: /(.*)\n    static_files: site/\\1/index.html\n    upload: site/(.*)\n</code></pre> <p>The .gitlab-ci file needs to have a deploy stage, a validate stage can be added if necessary. In this we will need to pull in the submodules and authenticate to GCP. To do this you will need to have a Service account key held in your CI/CD variables (For best practice, link this variable to a secret key in your software development platform).</p> <pre><code>.gcp-authentication: &amp;gcp-authentication\n    - echo $GCP_SERVICE_ACCOUNT_KEY &gt; gcloud-service-key.json \n    - gcloud auth activate-service-account --key-file gcloud-service-key.json &amp;&amp; rm gcloud-service-key.json\n</code></pre> <p>To pull in submodules you need the code below, this will pull the latest code from the submodules into the CI/CD stage.</p> <pre><code>variables:\n    GIT_SUBMODULE_STRATEGY: recursive\n\n.pull-submodules: &amp;pull-submodules\n    - git submodule sync --recursive\n    - git submodule update --remote --recursive\n</code></pre> <p>Now to deploy the site you need the following steps run on the cloud sdk image.</p> <ul> <li>Install libraries <code>pip install -r requirements.txt</code></li> <li>pull submodules <code>*pull-submodules</code></li> <li>build documentation pages into site folder ready for deployment <code>mkdocs build</code></li> <li>Authenticate to gcp <code>*gcp-authentication</code></li> <li>Deploy app in App Engine <code>gcloud app deploy app.yaml --project $GCP_PROJECT_ID --quiet</code></li> </ul> <pre><code>variables:\n    GIT_SUBMODULE_STRATEGY: recursive\n\nstages:\n- deploy\n\n.gcp-authentication: &amp;gcp-authentication\n    - echo $GCP_SERVICE_ACCOUNT_KEY &gt; gcloud-service-key.json \n    - gcloud auth activate-service-account --key-file gcloud-service-key.json &amp;&amp; rm gcloud-service-key.json\n\n.pull-submodules: &amp;pull-submodules\n    - git submodule sync --recursive\n    - git submodule update --remote --recursive\n\ndeploy_static_site:\n  stage: deploy \n  image: google/cloud-sdk\n  script:\n    - pip install -r requirements.txt\n    - *pull-submodules\n    - mkdocs build\n    - *gcp-authentication\n    - gcloud app deploy app.yaml --project $GCP_PROJECT_ID --quiet\n</code></pre> <p>The above will create the starting point to deploy a site from your central repository. Below I have added a split if you want to deploy two apps, one for develop and one for production. The sed command allows for a change in the app name within the yaml file. Another solution is to use a dev/prod app.yaml file.</p> <pre><code>variables:\n    GIT_SUBMODULE_STRATEGY: recursive\n\nstages:\n- deploy\n\n.gcp-authentication: &amp;gcp-authentication\n    - echo $GCP_SERVICE_ACCOUNT_KEY &gt; gcloud-service-key.json \n    - gcloud auth activate-service-account --key-file gcloud-service-key.json &amp;&amp; rm gcloud-service-key.json\n\n.pull-submodules: &amp;pull-submodules\n    - git submodule sync --recursive\n    - git submodule update --remote --recursive\n\ndeploy_static_site_dev:\n  stage: deploy \n  image: google/cloud-sdk\n  script:\n    - pip install -r requirements.txt\n    - *pull-submodules\n    - mkdocs build\n    - *gcp-authentication\n    - sed -i 's/${APP_NAME}/central-docs-dev/g' app.yaml &amp;&amp; gcloud app deploy app.yaml --project $GCP_PROJECT_ID --quiet\n  only:\n    refs:\n      - develop\n\ndeploy_static_site:\n  stage: deploy \n  image: google/cloud-sdk\n  script:\n    - pip install -r requirements.txt\n    - *pull-submodules\n    - mkdocs build\n    - *gcp-authentication\n    - sed -i 's/${APP_NAME}/central-docs/g' app.yaml &amp;&amp; gcloud app deploy app.yaml --project $GCP_PROJECT_ID --quiet\n  only:\n    refs:\n      - main\n</code></pre> <p>Step 3: To trigger on demand updates to the central repository from the child repository a step is needed within the child repository's CI/CD pipeline. Before the trigger I want validation of the mkdocs.yml file to ensure it wont break the central site. This is achieved by validating the yaml and doing a test build on the child repository's documentation.</p> <pre><code>validate_yaml:\n    stage: validate\n    image: sdesbure/yamllint\n    tags:\n        - validate\n    script:\n        - yamllint mkdocs.yml\n\nvalidate_build:\n    stage: validate\n    image: google/cloud-sdk\n    tags:\n        - validate\n    script:\n        - pip install mkdocs\n        - mkdocs build\n</code></pre> <p>Add a release stage to trigger the central repository CI/CD pipeline. I have added the variables UPDATE_DOCS &amp; DOCS_PROJECT_NAME for future use in the central repository.</p> <pre><code>validate_yaml:\n    stage: validate\n    image: sdesbure/yamllint\n    tags:\n        - validate\n    script:\n        - yamllint mkdocs.yml\n\nvalidate_build:\n    stage: validate\n    image: google/cloud-sdk\n    tags:\n        - validate\n    script:\n        - pip install mkdocs\n        - mkdocs build\n\nupdate:references:\n    trigger:\n    branch: 'main'\n    project: central-repo-name\n    strategy: depend\n    stage: release\n    only:\n        - main\n    variables:\n        UPDATE_DOCS: 'true'\n        DOCS_PROJECT_NAME: '${CI_PROJECT_NAME}'\n</code></pre> <pre><code>    only:\n        - main\n</code></pre> <p>The <code>only:</code> code in the yaml ensures this step is only run when a push is made to the main branch. The central repository should only contain production documentation.</p> <p>Step 4: Now push a documentation change to your main branch in your child repository. And sit back relax and wait for deployment. I have added an example of a CI/CD flow in the picture below. Every time a change is pushed to the main branch in your child repository it will trigger the central repo to update and refresh the central site.</p> <p></p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#accessing-the-central-site","title":"Accessing the Central Site","text":"<p>Step 1: Once the CI/CD pipeline has passed you'll be able to see the deployment in App Engine. You'll be able to see the app on the services page. Click on the app name to open up the URL. You will be able to access the site as IAP on the site has likely not been enabled.</p> <p></p> <p>Step 2: Go to the IAP page and the new application will appear under the name default. Enable IAP by toggling the IAP button, once enabled you will be able to set permissions.</p> <p></p> <p>Step 3: If you go back to the app you should see the consent screen we made earlier restricting you access to the site. To provide users access to the site you need to add them within the IAP page.</p> <p></p> <p>Step 4: This next step will need IAP admin privileges. In the IAP page, if you click on the app a panel will appear giving you access options. Add users to the site by granting them the IAP-secured Web App User permission. This can be at a user level, google groups level or domain level. You can choose how you control access to the site. It may take a few minutes for updates to be applied.</p> <p>You are all setup!</p> <p>When you go back to your app's URL you will have access to roam the site. Your centralised documentation site is up and running ready to serve your users.</p>"},{"location":"Articles/Documentation%20Series/central_docs_technical/#summary","title":"Summary","text":"<p>I hope this has helped you get started with creating a centralised documentation site for yourself and/or your team. </p> <p>What's exciting about this setup is its potential for expansion creating some brilliant automated documentation. I will be adding more information on this topic in future articles.</p>"},{"location":"Home/Markdown/","title":"What is Markdown and why should we use it?","text":"<p>Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Markdown is a fast and easy way to take notes, create content for a website, and produce print-ready documents. If you use GitHub or Gitlab, you'll be familiar with the \u201creadme.md\u201d files that show up in the root of a repository. That \u201cmd\u201d stands for markdown. Markdown allows you to effectively communicate with others in your Gitlab issues, pull requests, comments, and documentation. When integrated with Gitlab the collaboration aspect of markdown makes it very powerful and a must for tech teams.</p>"},{"location":"Home/Markdown/#why-use-markdown","title":"Why Use Markdown?","text":"<p>You might be wondering why people use Markdown instead of a WYSIWYG editor. Why write with Markdown when you can press buttons in an interface to format your text? As it turns out, there are several reasons why people use Markdown instead of WYSIWYG editors.</p> <ul> <li> <p>Markdown can be used for everything. People use it to create websites, documents, notes, books, presentations, email messages, and technical documentation.</p> </li> <li> <p>Markdown is portable. Files containing Markdown-formatted text can be opened using virtually any application. If you decide you don\u2019t like the Markdown application you\u2019re currently using, you can import your Markdown files into another Markdown application. That\u2019s in stark contrast to word processing applications like Microsoft Word that lock your content into a proprietary file format.</p> </li> <li> <p>Markdown is platform independent. You can create Markdown-formatted text on any device running any operating system.</p> </li> <li> <p>Markdown is future proof. Even if the application you\u2019re using stops working at some point in the future, you\u2019ll still be able to read your Markdown-formatted text using a text editing application. This is an important consideration when it comes to books, university theses, and other milestone documents that need to be preserved indefinitely.</p> </li> <li> <p>Markdown is everywhere. Websites like Reddit and GitHub support Markdown, and lots of desktop and web-based applications support it.</p> </li> </ul>"},{"location":"Home/Markdown/#ease-of-use-and-barrier-to-entry","title":"Ease of Use and Barrier to Entry","text":"<p>Markdown is the defacto choice for writing software documentation. From open source contributions to private repositories for enterprises, every single README file is written in markdown (i.e. <code>README.md</code>). Markdown gets rid of all the distractions of a formatting toolbar and mouse clicks by helping you focus on your writing without lifting your fingers off of the keyboard. It does have a small learning curve when using advanced features, however the resources are so abundant you're only a Google search away from achieving anythign you can imagine.</p>"},{"location":"Home/Markdown/#useful-resources","title":"Useful Resources","text":"<p>Have a look at the following resources for quick information on Markdown and how to use it!</p> <ul> <li>Getting Started</li> <li>Cheat Sheet</li> <li>Basic Syntax</li> <li>Extended Syntax</li> <li>GitLab Flavoured Markdown (GLFM)</li> <li>GitHub Flavoured Markdown (GFM) Spec</li> <li>Markdown and Visual Studio Code</li> <li>VS Code Markdown Guide</li> </ul>"},{"location":"Home/Markdown/basic_cheat_sheet/","title":"Markdown Cheat Sheet","text":"<p>This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can\u2019t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax.</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#basic-syntax","title":"Basic Syntax","text":"<p>These are the elements outlined in John Gruber\u2019s original design document. All Markdown applications support these elements.</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#heading","title":"Heading","text":"<pre><code># H1\n## H2\n### H3\n</code></pre>"},{"location":"Home/Markdown/basic_cheat_sheet/#h1","title":"H1","text":""},{"location":"Home/Markdown/basic_cheat_sheet/#h2","title":"H2","text":""},{"location":"Home/Markdown/basic_cheat_sheet/#h3","title":"H3","text":""},{"location":"Home/Markdown/basic_cheat_sheet/#bold","title":"Bold","text":"<pre><code>**bold text**\n</code></pre> <p>bold text</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#italic","title":"Italic","text":"<pre><code>*italicized text*\n</code></pre> <p>italicized text</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#blockquote","title":"Blockquote","text":"<pre><code>&gt; blockquote\n</code></pre> <p>blockquote</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#ordered-list","title":"Ordered List","text":"<pre><code>1. First item\n2. Second item\n3. Third item\n</code></pre> <ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"Home/Markdown/basic_cheat_sheet/#unordered-list","title":"Unordered List","text":"<pre><code>- First item\n- Second item\n- Third item\n</code></pre> <ul> <li>First item</li> <li>Second item</li> <li>Third item</li> </ul>"},{"location":"Home/Markdown/basic_cheat_sheet/#code","title":"Code","text":"<pre><code>`code`\n</code></pre> <p><code>code</code></p>"},{"location":"Home/Markdown/basic_cheat_sheet/#horizontal-rule","title":"Horizontal Rule","text":"<pre><code>---\n</code></pre>"},{"location":"Home/Markdown/basic_cheat_sheet/#link","title":"Link","text":"<pre><code>[Markdown Guide](https://www.markdownguide.org)\n</code></pre> <p>Markdown Guide</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#image","title":"Image","text":"<pre><code>![alt text](https://www.markdownguide.org/assets/images/tux.png)\n</code></pre>"},{"location":"Home/Markdown/basic_cheat_sheet/#extended-syntax","title":"Extended Syntax","text":"<p>These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements.</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#table","title":"Table","text":"<pre><code>| Syntax | Description |\n| ----------- | ----------- |\n| Header | Title |\n| Paragraph | Text |\n</code></pre> Syntax Description Header Title Paragraph Text"},{"location":"Home/Markdown/basic_cheat_sheet/#fenced-code-block","title":"Fenced Code Block","text":"<pre><code>  ```\n  {\n    \"firstName\": \"John\",\n    \"lastName\": \"Smith\",\n    \"age\": 25\n  }\n  ```\n</code></pre> <pre><code>{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\",\n  \"age\": 25\n}\n</code></pre>"},{"location":"Home/Markdown/basic_cheat_sheet/#footnote","title":"Footnote","text":"<pre><code>Here's a sentence with a footnote. [^1]\n\n[^1]: This is the footnote.\n</code></pre> <p>Here's a sentence with a footnote. <sup>1</sup></p>"},{"location":"Home/Markdown/basic_cheat_sheet/#heading-id","title":"Heading ID","text":"<pre><code>### My Great Heading {#custom-id}\n</code></pre>"},{"location":"Home/Markdown/basic_cheat_sheet/#custom-id","title":"My Great Heading","text":""},{"location":"Home/Markdown/basic_cheat_sheet/#definition-list","title":"Definition List","text":"<pre><code>term\n: definition\n</code></pre> term definition"},{"location":"Home/Markdown/basic_cheat_sheet/#strikethrough","title":"Strikethrough","text":"<pre><code>~~The world is flat.~~\n</code></pre> <p>The world is flat.</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#task-list","title":"Task List","text":"<pre><code>- [x] Write the press release\n- [ ] Update the website\n- [ ] Contact the media\n</code></pre> <ul> <li> Write the press release</li> <li> Update the website</li> <li> Contact the media</li> </ul>"},{"location":"Home/Markdown/basic_cheat_sheet/#emoji","title":"Emoji","text":"<pre><code>That is so funny! :joy:\n</code></pre> <p>That is so funny! </p> <p>(See also Copying and Pasting Emoji or press Win+.)</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#highlight","title":"Highlight","text":"<pre><code>I need to highlight these ==very important words==.\n</code></pre> <p>I need to highlight these very important words.</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#subscript","title":"Subscript","text":"<pre><code>H~2~O\n</code></pre> <p>H<sub>2</sub>O</p>"},{"location":"Home/Markdown/basic_cheat_sheet/#superscript","title":"Superscript","text":"<pre><code>X^2^\n</code></pre> <p>X<sup>2</sup></p> <ol> <li> <p>This is the footnote.\u00a0\u21a9</p> </li> </ol>"},{"location":"Home/Markdown/formatting_examples/","title":"Formatting Examples","text":"<p>Click here for more examples</p>"},{"location":"Home/Markdown/formatting_examples/#admonitions","title":"Admonitions","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Bug</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"Home/Markdown/formatting_examples/#buttons","title":"Buttons","text":"<p>Check Out Our Repo</p>"},{"location":"Home/Markdown/formatting_examples/#code-blocks","title":"Code Blocks","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"Home/Markdown/formatting_examples/#content-tabs","title":"Content tabs","text":"Unordered listOrdered list <ul> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ul> <ol> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ol> <p>With code blocks</p> CC++ <pre><code>#include &lt;stdio.h&gt;\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"Home/Markdown/formatting_examples/#data-tables","title":"Data Tables","text":"Method Description <code>GET</code> \u2705     Fetch resource <code>PUT</code> \u2796 Update resource <code>DELETE</code> \u274e    Delete resource"},{"location":"Home/Markdown/formatting_examples/#word-formatting","title":"Word Formatting","text":"<ul> <li>This was marked</li> <li>This was inserted</li> <li>This was deleted</li> </ul> <p>Ctrl+Alt+Del</p>"},{"location":"Home/Markdown/formatting_examples/#images","title":"Images","text":"<p> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"Home/Markdown/formatting_examples/#math","title":"Math","text":"\\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such that \\(f(a)=f(b)\\).</p>"},{"location":"Home/Markdown/getting_started/","title":"Getting Started","text":""},{"location":"Home/Markdown/getting_started/#setup","title":"Setup","text":"<p>So you have read the intro and agree that documentation as code is the way forward and that writing in markdown will help us get there. Now you want to start using markdown to write your documentation as code. So what is needed to get started? Our current Tech Stack includes VS Code &amp; GitLab, both of these allow for viewing and editing markdown files. But, there are other programs you can use as well.</p> <ul> <li>VS Code (+ recommended extensions)</li> <li>Markdown All in One</li> <li>markdownlint</li> <li>Markdown Extended</li> <li>Markdown Preview Mermaid Support</li> <li>Doc Writer Profile Template (all-in-one)</li> <li>Code Spell Checker</li> <li>Obsidian</li> <li>logseq</li> <li>MarkText</li> <li>ghostwriter</li> </ul> <p>You can use WSL or Windows to write and edit your code. Ensure that if you are using WSL &amp; VSCode that you install the extensions within WSL VS code.</p>"},{"location":"Home/Markdown/getting_started/#writing-your-docs","title":"Writing your docs","text":"<p>Once you have a programme setup to edit and view your markdown files in you can start to write your documentation. VS Code is better to use with repos as you can clone your repos and write your md files straight into the repo, adding collaboration to your documentation. Other tools such as Obsidian are great for writing locally documentation for yourself, (equally you could still use VS Code for this as well).</p> <ol> <li>Think about folder structure, structure is important for keeping organised notes</li> <li>all markdown files are suffixed with .md</li> <li>To Preview your docs in VS Code press Ctrl+Shift+V</li> </ol>"},{"location":"Home/mkdocs/mkdocs_grids/","title":"MKDocs Grids","text":"<p>Grids are great for building index pages that show a brief overview of a large section of your documentation. Below I have highlighted some of the ways that the grid option can be used as part of Mkdocs Material, providing good visual information.</p>"},{"location":"Home/mkdocs/mkdocs_grids/#card-grids","title":"Card Grids","text":"<p>We use \"grid cards\" class and fontawesome-brands-\"brandname\" to give visually appearing card grids. This approach works only fontawesome has expected brands and it can be found here.</p>"},{"location":"Home/mkdocs/mkdocs_grids/#syntax_1","title":"syntax_1:","text":"<pre><code>&lt;div class=\"grid cards\" markdown&gt;\n- :fontawesome-brands-\"brandname\": __\"Content to highlight\"__ for content and structure\n&lt;/div&gt;\n</code></pre> <p>Example:</p> <pre><code>&lt;div class=\"grid cards\" markdown&gt;\n- :fontawesome-brands-html5: __HTML__ for content and structure\n- :fontawesome-brands-js: __JavaScript__ for interactivity\n- :fontawesome-brands-css3: __CSS__ for text running out of boxes\n- :fontawesome-brands-internet-explorer: __Internet Explorer__ ... huh?\n- :fontawesome-brands-youtube: __google cloud__ for data warehouse\n&lt;/div&gt;\n</code></pre> <ul> <li> HTML for content and structure</li> <li> JavaScript for interactivity</li> <li> CSS for text running out of boxes</li> <li> Internet Explorer ... huh?</li> <li> YouTube for watching videos.</li> </ul>"},{"location":"Home/mkdocs/mkdocs_grids/#syntax_2","title":"syntax_2:","text":"<pre><code>&lt;div class=\"grid cards\" markdown&gt;\n- :fontawesome-brands-markdown:{ .lg .middle } __It's just Markdown__\n  \"Card Contents\"\n  \"Reference/Links\"\n&lt;/div&gt;\n</code></pre> <p>Example:</p> <pre><code>&lt;div class=\"grid cards\" markdown&gt;\n-   :fontawesome-brands-markdown:{ .lg .middle } __It's just Markdown__\n    ---\n    Focus on your content and generate a responsive and searchable static site\n    [:octicons-arrow-right-24: Markdown](https://www.markdownguide.org/)\n\n&lt;/div&gt;\n</code></pre> <ul> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Markdown</p> </li> </ul>"},{"location":"Home/mkdocs/mkdocs_grids/#syntax_3","title":"syntax_3:","text":"<p>title</p><p>Content fo the title</p> <p>Example:</p> <pre><code>::cards:: image-bg\n\n- title: Markdown\n  content: Focus on your content and generate a responsive and searchable static site.\n  image: ../../mkdocs/images/markdown_mark.png\n  url: https://www.markdownguide.org/\n\n::/cards::\n</code></pre> <p>Markdown</p><p>Focus on your content and generate a responsive and searchable static site.</p> <p>Markdown2</p><p>Focus on your content and generate a responsive and searchable static site.</p>"},{"location":"Home/mkdocs/mkdocs_material/","title":"Material for Mkdocs","text":""},{"location":"Home/mkdocs/mkdocs_material/#what-is-it","title":"What is it?","text":"<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Material is a version of mkdocs which provides added customisation and extensions that you cannot get on regular Mkdocs.</p> <p>The main benefits of using material include it being fast, lightweight, open source and built for everyone. The main benefit of material is the plugins that you can use with the site. This includes a mermaid2 plugin, mono-repo/multi-repo plugins etc, more plugins will be added as the site grow. See below for available plugins with material.</p> <p>Material for Mkdocs site</p>"},{"location":"Home/mkdocs/mkdocs_material/#trusted-in-the-industry","title":"Trusted in the industry","text":"<p>Industry leaders, as well as many successful Open Source projects, rely on Material for MkDocs to create professional and beautiful documentation \u2013 no frontend experience required. Choose a mature and actively maintained solution and start writing in minutes.</p> <p>We are currently not a sponsor so only have the open source package. However, this is more than enough to utilise documentation as code efficiently.</p>"},{"location":"Home/mkdocs/mkdocs_material/#getting-started","title":"Getting started","text":"<p>Material for MkDocs is a powerful documentation framework on top of MkDocs, a static site generator for project documentation.<sup>1</sup> If you're familiar with  Python, you can install Material for MkDocs with <code>pip</code>, the Python package manager. If not, we recommend using <code>docker</code>.</p>"},{"location":"Home/mkdocs/mkdocs_material/#installation","title":"Installation","text":""},{"location":"Home/mkdocs/mkdocs_material/#with-pip","title":"with pip recommended","text":"<p>Material for MkDocs is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install Material for MkDocs with:</p> Latest9.x <pre><code>pip install mkdocs-material\n</code></pre> <pre><code>pip install mkdocs-material==\"9.*\" # (1)!\n</code></pre> <ol> <li> <p>Material for MkDocs uses semantic versioning<sup>2</sup>, which is why it's a     good idea to limit upgrades to the current major version.</p> <p>This will make sure that you don't accidentally [upgrade to the next major version], which may include breaking changes that silently corrupt your site. Additionally, you can use <code>pip freeze</code> to create a lockfile, so builds are reproducible at all times:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>Now, the lockfile can be used for installation:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> </ol> <p>This will automatically install compatible versions of all dependencies: MkDocs, Markdown, Pygments and Python Markdown Extensions. Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately.</p> <p> How to set up Material for MkDocs by @james-willett \u2013  15m \u2013 Learn how to create and host a documentation site using Material for  MkDocs on GitHub Pages in a step-by-step guide.</p> <p>Tip: If you don't have prior experience with Python, we recommend reading  Using Python's pip to Manage Your Projects' Dependencies, which is a really good introduction on the mechanics of Python package management and helps you troubleshoot if you run into errors.</p> <ol> <li> <p>In 2016, Material for MkDocs started out as a simple theme for MkDocs, but over the course of several years, it's now much more than that \u2013 with the many built-in plugins, settings, and countless customization abilities, Material for MkDocs is now one of the simplest and most powerful frameworks for creating documentation for your project.\u00a0\u21a9</p> </li> <li> <p>Note that improvements of existing features are sometimes released as patch releases, like for example improved rendering of content tabs, as they're not considered to be new features.\u00a0\u21a9</p> </li> </ol>"},{"location":"Home/mkdocs/mkdocs_mermaid/","title":"MKDocs Mermaid","text":""},{"location":"Home/mkdocs/mkdocs_mermaid/#examples","title":"Examples","text":"<pre><code>  ```mermaid\n  graph TD;\n  A--&gt;B;\n  A--&gt;C;\n  B--&gt;D;\n  C--&gt;D;\n  ```\n</code></pre> <pre><code>graph TD;\n  A--&gt;B;\n  A--&gt;C;\n  B--&gt;D;\n  C--&gt;D;</code></pre> <pre><code>  ```mermaid\n  graph TB\n\n    SubGraph1 --&gt; SubGraph1Flow\n    subgraph \"SubGraph 1 Flow\"\n    SubGraph1Flow(SubNode 1)\n    SubGraph1Flow -- Choice1 --&gt; DoChoice1\n    SubGraph1Flow -- Choice2 --&gt; DoChoice2\n    end\n\n    subgraph \"Main Graph\"\n    Node1[Node 1] --&gt; Node2[Node 2]\n    Node2 --&gt; SubGraph1[Jump to SubGraph1]\n    SubGraph1 --&gt; FinalThing[Final Thing]\n  end\n  ```\n</code></pre> <pre><code>graph TB\n\n  SubGraph1 --&gt; SubGraph1Flow\n  subgraph \"SubGraph 1 Flow\"\n  SubGraph1Flow(SubNode 1)\n  SubGraph1Flow -- Choice1 --&gt; DoChoice1\n  SubGraph1Flow -- Choice2 --&gt; DoChoice2\n  end\n\n  subgraph \"Main Graph\"\n  Node1[Node 1] --&gt; Node2[Node 2]\n  Node2 --&gt; SubGraph1[Jump to SubGraph1]\n  SubGraph1 --&gt; FinalThing[Final Thing]\nend</code></pre> <pre><code>  ``` mermaid\n  sequenceDiagram\n    autonumber\n    Alice-&gt;&gt;John: Hello John, how are you?\n    loop Healthcheck\n        John-&gt;&gt;John: Fight against hypochondria\n    end\n    Note right of John: Rational thoughts!\n    John--&gt;&gt;Alice: Great!\n    John-&gt;&gt;Bob: How about you?\n    Bob--&gt;&gt;John: Jolly good!\n  ```\n</code></pre> <pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre> <pre><code>  ``` mermaid\n  classDiagram\n    Person &lt;|-- Student\n    Person &lt;|-- Professor\n    Person : +String name\n    Person : +String phoneNumber\n    Person : +String emailAddress\n    Person: +purchaseParkingPass()\n    Address \"1\" &lt;-- \"0..1\" Person:lives at\n    class Student{\n      +int studentNumber\n      +int averageMark\n      +isEligibleToEnrol()\n      +getSeminarsTaken()\n    }\n    class Professor{\n      +int salary\n    }\n    class Address{\n      +String street\n      +String city\n      +String state\n      +int postalCode\n      +String country\n      -validate()\n      +outputAsLabel()  \n    }\n  ```\n</code></pre> <pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre>"},{"location":"airflow-helper/","title":"Airflow Helper","text":"<p>This is a starter code template, with helper code. The code here is to help you set up you own repo, not to be used as a repo itself. This code is to help you get started with initialising a local aiflow instance. A local airflow instance is useful for testing and debugging your airflow code. However, it should not be used for production deployments.</p> <p>If you are using windows you will need to first install WSL2 and get your WSL instance setup.</p>"},{"location":"airflow-helper/#initial-wsl-and-project-setup","title":"Initial WSL and project setup","text":"<ol> <li>Make sure you have a working WSL terminal (WSL2).</li> <li> <p>Gitbash installed https://git-scm.com/downloads</p> </li> <li> <p>Clone the repository to your WSL terminal.</p> </li> <li> <p>Install just. Just is built using the Rust programming language (just git repo). There are installation instructions in the repo. We have found the most success using <code>cargo</code>. You will also need to run <code>sudo apt install build-essential</code> this is to ensure you have a C linker installed. To install cargo you can use rustup to install the rust toolchain (which cargo is a part of). Once cargo is installed (ensure you follow the last command for setup <code>. \"$HOME/.cargo/env\"</code>) you can run <code>cargo install just</code></p> </li> <li> <p>Either run <code>just project-setup</code> or <code>just full-project-setup</code>.  If this is your first time on WSL, <code>full-project-setup</code> will: </p> </li> <li> <p>Install all the necessary packages for package manager</p> </li> <li>Install Python3</li> <li>configure Application Default Credentials</li> <li>Initialise SDK environment</li> <li>Install Docker</li> <li>Install Lazydocker</li> <li>Setup virtual environment and install libraries from requirements-dev.txt</li> <li>Create .env file</li> </ol> <p><code>project-setup</code> will do the project specific installation:</p> <ul> <li>Setup virtual environment and install libraries from requirements-dev.txt</li> <li>Install project specific python package</li> <li>Create .env file</li> </ul> <p>You are now ready to spin up an airflow instance \ud83d\ude80 see docs/airflow_setup for more details</p>"},{"location":"airflow-helper/airflow_setup/","title":"Development Container vs Docker","text":"<p>I have found that the best 2 ways to run/setup an airflow instance are with Development Containers (Dev Containers) or Docker. With the docker solution offering more flexibility and control and the Dev Containers solution offering simplicity.</p> <p>I have found the docker solution to be more useful, mostly because I have paired it with justfile commands to make the process simple and with lazydocker I can fully understand how my instance is running and all the errors associated with it. However, the dev container method does give you a clean repo and is also easy to maintain. The choice is personal preference. This repo contains code for both solutions and I have documented the process below.</p> <p>I have also written an article on the topic here</p>"},{"location":"airflow-helper/airflow_setup/#running-airflow-with-a-development-container","title":"Running airflow with a Development Container","text":"<p>The project contains a configuration files for Visual Studio Code Development Containers in the <code>.devcontainer/</code> directory. To run the project with this locally, follow the below steps:</p> <ol> <li>With the project open in VSCode open the folder in the development container by clicking the prompt in the bottom right corner or running \"Reopen in Container\" command via Command Palette (usually accessible with Ctrl+Shift+P).</li> <li>First time starting the containers can be slow due to building the docker images, after it has opened validate it is working by navigating to localhost:8080 in your browser to find the airflow log in page (default username and password is 'airflow').</li> </ol>"},{"location":"airflow-helper/airflow_setup/#customising-the-development-container","title":"Customising the Development Container","text":"<p>Common changes you may wish to make:</p> <ul> <li>Any additional pip requirements you have can be added to <code>requirements.txt</code> or <code>requirements-dev.txt</code>.</li> <li>In the <code>.devcontainer/docker-compose.yml</code> file:</li> <li>Set <code>PYTHON_VARIANT</code> variable to the version of python you wish to use e.g. '3.10'</li> <li>Set <code>AIRFLOW_VERSION</code> variable to the version of airflow you wish to use e.g. '2.4.2'.</li> <li>Set <code>GOOGLE_CLOUD_PROJECT</code> variable to GCP project you are using.</li> <li>Add any additional airflow providers to <code>AIRFLOW_PROVIDERS</code>.</li> <li>In the <code>.devcontainer/.env</code> file (available after building container for first time):<ul> <li>Update <code>GCP_CREDENTIALS_LOCAL</code> to point at different location for a GCP credential file.</li> </ul> </li> </ul> <p>After making any of the above changes you will need to rebuid the container by running \"Rebuild Container\" command via Command Palette (usually accessible with Ctrl+Shift+P).</p>"},{"location":"airflow-helper/airflow_setup/#running-airflow-with-docker","title":"Running Airflow with Docker","text":"<p>To run airflow using Docker, instead Run <code>local-airflow</code> to create a local airflow instance for local testing.</p> <p>Large airflow instance with redis, workers, triggerer, scheduler, postgres and webserer <pre><code>just local-airflow\n</code></pre> small airflow instance with postgres and webserver <pre><code>just local-airflow-lite\n</code></pre></p> <p>To shutdown the airflow instance run <pre><code>just destroy-airflow\n</code></pre> or <pre><code>just destroy-airflow-lite\n</code></pre></p> <p>The just file also has extra commands for docker &amp; installation <pre><code># Docker commands\njust clean-docker-containers\njust force-remove-docker-containers\njust show-running-containers\njust show-all-containers\n\n# Install requirements.txt on Virtual Environment\njust install-libraries\njust uninstall-libraries\n</code></pre></p>"},{"location":"airflow-helper/airflow_setup/#automated-testing","title":"Automated testing","text":"<ul> <li>In <code>docs/automation/data_dictionary</code> Edit line 56 to link to production datasets for data dictionary creation</li> <li> <p>In justfile update-unit-test-docs section &amp; update-python-docs add any folders or python scripts that you would like to be documented by lazydocs</p> </li> <li> <p>Run <code>just update-docs</code> to update all docs (python, unit tests, data dictonaries)</p> </li> <li> <p>Specific tests can be run using <code>just update-python-docs</code> <code>update-unit-test-docs</code> <code>update-data-dictionary-docs</code></p> </li> </ul>"},{"location":"airflow-helper/overview/","title":"Airflow Helper","text":"<p>Airflow Repository</p> <p>This is a starter code template, with helper code. The code here is to help you set up you own repo, not to be used as a repo itself. This code is to help you get started with initialising a local aiflow instance. A local airflow instance is useful for testing and debugging your airflow code. However, it should not be used for production deployments.</p> <p>If you are using windows you will need to first install WSL2 and get your WSL instance setup.</p>"},{"location":"airflow-helper/overview/#initial-wsl-and-project-setup","title":"Initial WSL and project setup","text":"<ol> <li> <p>Make sure you have a working WSL terminal (WSL2).</p> <ul> <li>Gitbash installed https://git-scm.com/downloads</li> </ul> </li> <li> <p>Clone the repository to your WSL terminal.</p> </li> <li> <p>Install just. Just is built using the Rust programming language (just git repo). There are installation instructions in the repo. We have found the most success using <code>cargo</code>. You will also need to run <code>sudo apt install build-essential</code> this is to ensure you have a C linker installed. To install cargo you can use rustup to install the rust toolchain (which cargo is a part of). Once cargo is installed (ensure you follow the last command for setup <code>. \"$HOME/.cargo/env\"</code>) you can run <code>cargo install just</code></p> </li> <li> <p>Either run <code>just project-setup</code> or <code>just full-project-setup</code>. </p> </li> </ol> <p>If this is your first time on WSL, <code>full-project-setup</code> will: </p> <ul> <li>Install all the necessary packages for package manager</li> <li>Install Python3</li> <li>configure Application Default Credentials</li> <li>Initialise SDK environment</li> <li>Install Docker</li> <li>Install Lazydocker</li> <li>Setup virtual environment and install libraries from requirements-dev.txt</li> <li>Create .env file</li> </ul> <p><code>project-setup</code> will do the project specific installation:</p> <ul> <li>Setup virtual environment and install libraries from requirements-dev.txt</li> <li>Install project specific python package</li> <li>Create .env file</li> </ul> <p>You are now ready to spin up an airflow instance \ud83d\ude80 see airflow_setup for more details</p>"},{"location":"airflow-helper/the_dag_code/","title":"Useful Code for Dag Runs","text":""},{"location":"airflow-helper/the_dag_code/#settings-configuration","title":"Settings Configuration","text":"<p>Configuration structures for Airflow pipeline.</p>"},{"location":"airflow-helper/the_dag_code/#config.settings.PipelineConfiguration","title":"<code>PipelineConfiguration</code>  <code>dataclass</code>","text":"<p>Configuration for Airflow pipeline.</p> <p>Attributes:</p> Name Type Description <code>gcp_project</code> <code>str</code> <p>GCP project that pipeline runs in.</p> <code>bigquery_dataset</code> <code>str</code> <p>BigQuery dataset used by project.</p> <code>bucket</code> <code>str</code> <p>Bucket used by project.</p> <code>dataform_repository</code> <code>str</code> <p>Dataform repository to use.</p> <code>dataform_region</code> <code>str</code> <p>Region dataform repository sits in.</p> <code>dataform_branch</code> <code>str</code> <p>Branch of dataform repository to run.</p> <code>generic_account_username</code> <code>str</code> <p>Generic account username.</p> <code>generic_account_password_variable_name</code> <code>str</code> <p>Variable containing the password for the account.</p> Source code in <code>docs/airflow-helper/config/settings.py</code> <pre><code>@dataclass\nclass PipelineConfiguration:\n    \"\"\"Configuration for Airflow pipeline.\n\n    Attributes:\n        gcp_project (str):\n            GCP project that pipeline runs in.\n        bigquery_dataset (str):\n            BigQuery dataset used by project.\n        bucket (str):\n            Bucket used by project.\n        dataform_repository (str):\n            Dataform repository to use.\n        dataform_region (str):\n            Region dataform repository sits in.\n        dataform_branch (str):\n            Branch of dataform repository to run.\n\n        generic_account_username (str):\n            Generic account username.\n        generic_account_password_variable_name (str):\n            Variable containing the password for the account.\n    \"\"\"\n\n    gcp_project: str\n    bigquery_dataset: str\n    bucket: str\n    dataform_repository: str\n    dataform_region: str\n    dataform_branch: str\n</code></pre>"},{"location":"airflow-helper/the_dag_code/#config.settings.get_pipeline_config","title":"<code>get_pipeline_config()</code>","text":"<p>Get pipeline configuration for current environment.</p> <p>Determines which pipeline configuration to used based on APP_ENV environment variable.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching pipeline for APP_ENV.</p> <p>Returns:</p> Name Type Description <code>PipelineConfiguration</code> <code>PipelineConfiguration</code> <p>Configuration for current environment.</p> Source code in <code>docs/airflow-helper/config/settings.py</code> <pre><code>def get_pipeline_config() -&gt; PipelineConfiguration:\n    \"\"\"Get pipeline configuration for current environment.\n\n    Determines which pipeline configuration to used based on\n    APP_ENV environment variable.\n\n\n    Raises:\n        ValueError: If no matching pipeline for APP_ENV.\n\n    Returns:\n        PipelineConfiguration: Configuration for current environment.\n    \"\"\"\n    app_env = os.getenv(\"APP_ENV\", \"development\")\n\n    if app_env == \"development\":\n        return development\n    if app_env == \"production\":\n        return production\n\n    raise ValueError(f\"Pipeline configuration not found for APP_ENV: {app_env}\")\n</code></pre>"},{"location":"airflow-helper/the_dag_code/#tasks","title":"Tasks","text":""},{"location":"airflow-helper/the_dag_code/#example_dag--example_dag","title":"example_dag","text":"<p>This an example dag used for getting started.</p>"},{"location":"airflow-helper/the_dag_code/#example_dag.example_task","title":"<code>example_task()</code>","text":"<p>An example task.</p> Source code in <code>docs/airflow-helper/example_dag.py</code> <pre><code>@task()\ndef example_task():\n    \"\"\"An example task.\"\"\"\n    pass\n</code></pre>"},{"location":"airflow-helper/the_dag_code/#example_dag.run_dataform","title":"<code>run_dataform()</code>","text":"<p>Compile and run the dataform pipeline.</p> Source code in <code>docs/airflow-helper/example_dag.py</code> <pre><code>@task_group()\ndef run_dataform():\n    \"\"\"Compile and run the dataform pipeline.\"\"\"\n    create_compilation_result = DataformCreateCompilationResultOperator(\n        task_id=\"create_compilation_result\",\n        retries=2,\n        project_id=settings.pipeline.gcp_project,\n        region=settings.pipeline.dataform_region,\n        repository_id=settings.pipeline.dataform_repository,\n        compilation_result={\n            \"git_commitish\": settings.pipeline.dataform_branch,\n        },\n    )\n\n    create_workflow_invocation = DataformCreateWorkflowInvocationOperator(\n        task_id=\"create_workflow_invocation\",\n        project_id=settings.pipeline.gcp_project,\n        region=settings.pipeline.dataform_region,\n        repository_id=settings.pipeline.dataform_repository,\n        workflow_invocation={\n            \"compilation_result\": \"{{ ti.xcom_pull('%s')['name'] }}\" # noqa: UP031\n            % (create_compilation_result.task_id)\n        },\n    )\n\n    create_compilation_result &gt;&gt; create_workflow_invocation\n</code></pre>"},{"location":"airflow-helper/automation/data_dictionary/","title":"Automating Data Dictionary creation and maintenance","text":"<p>Nobody wants to spend all their time writing documentation, so lets make some code to do it for you. I have written a script which takes tables from any bigquery dataset (that you have access to) and generates a data dictionary for each table in the dataset within a single md file. If you display this in a centralised site such as Mkdocs you can then split your data dictionaries by dataset and they are all easily searchable through the search bar on Mkdocs. This makes looking at data very powerful as its easy to search for certain key words over many different projects.</p> <p>Please see my previous article for more information on setting up a centralised documentation site.</p>"},{"location":"airflow-helper/automation/data_dictionary/#how-it-works","title":"How it works","text":"<p>This is specifically for extracting data information out of bigquery and displaying it as a data dictionary within a central documentation site.</p> <ol> <li>You need to ensure your columns descriptions are labelled appropriately. See here for information on how to do this within dataform: Centralising &amp; Automating Dataform Column Descriptions.</li> <li>You need to have access to the appropriate tables and access to the metadata tables: \"INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\"</li> <li>If your table schemas or descriptions change you will need to rerun this code and push to the repo.</li> <li>Have pandas &amp; google-cloud-bigquery installed to use the libraries</li> </ol>"},{"location":"airflow-helper/automation/data_dictionary/#the-code","title":"The Code","text":""},{"location":"airflow-helper/automation/data_dictionary/#automation.data_dictionary.bq_to_df","title":"<code>bq_to_df(project, dataset, table, columns)</code>","text":"<p>Extracts table from BigQuery and returns a dataframe of the table</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>project location of table</p> required <code>dataset</code> <code>str</code> <p>dataset location of table</p> required <code>table</code> <code>str</code> <p>table name of table</p> required <code>columns</code> <code>str</code> <p>columns to be extracted from table (split by comma)</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>dataframe</code> <p>dataframe of data within table for specified columns</p> Source code in <code>docs/airflow-helper/automation/data_dictionary.py</code> <pre><code>def bq_to_df(project, dataset, table, columns):\n    \"\"\"\n    Extracts table from BigQuery and returns a dataframe of the table\n\n    Args:\n        project (str): project location of table\n        dataset (str): dataset location of table\n        table (str): table name of table\n        columns (str): columns to be extracted from table (split by comma)\n\n    Returns:\n        df (dataframe): dataframe of data within table for specified columns\n    \"\"\"\n    client = bigquery.Client()\n    sql = f\"SELECT {columns} FROM `{project}.{dataset}.{table}`\"\n    df = client.query(sql).to_dataframe()\n    return df\n</code></pre>"},{"location":"airflow-helper/automation/data_dictionary/#automation.data_dictionary.create_md_file_from_string","title":"<code>create_md_file_from_string(string, filename)</code>","text":"<p>creates or opens a file based on a file name and writes/overwrites the data within the file with from the string input</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>string to be written into the file</p> required <code>filename</code> <code>str</code> <p>name and location of file to be written to</p> required Source code in <code>docs/airflow-helper/automation/data_dictionary.py</code> <pre><code>def create_md_file_from_string(string, filename):\n    \"\"\"\n    creates or opens a file based on a file name and writes/overwrites the data within the file with from the string input\n\n    Args:\n        string (str): string to be written into the file\n        filename (str): name and location of file to be written to\n    \"\"\"\n    with open(filename, \"w\") as file:\n        file.write(string)\n</code></pre>"},{"location":"airflow-helper/automation/data_dictionary/#automation.data_dictionary.extract_information_schema_to_markdown","title":"<code>extract_information_schema_to_markdown(project, dataset)</code>","text":"<p>Extract the information schema from BQ and translate to markdown</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>project location of table</p> required <code>dataset</code> <code>str</code> <p>dataset location of table</p> required Source code in <code>docs/airflow-helper/automation/data_dictionary.py</code> <pre><code>def extract_information_schema_to_markdown(project, dataset):\n    \"\"\"\n    Extract the information schema from BQ and translate to markdown\n\n    Args:\n        project (str): project location of table\n        dataset (str): dataset location of table\n    \"\"\"\n    columns = \"table_name, column_name, data_type, description, collation_name, rounding_mode\"\n    table = \"INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\"\n\n    df = bq_to_df(project, dataset, table, columns)\n    tables = df[\"table_name\"].unique()\n    md_string = f\"# {dataset} \\n \\n## Data Dictionary for {dataset} in {project} \\n\"\n    for t in tables:\n        filtered_df = df.loc[df[\"table_name\"] == t]\n        filtered_df = filtered_df.reset_index(drop=True)\n        table_string = filtered_df.to_markdown()\n        md_string = md_string + \"\\n\" + f\"### {t}\" + \"\\n\" + table_string + \"\\n\"\n    create_md_file_from_string(md_string, f\"docs/data_dictionary/{dataset}.md\")\n</code></pre>"},{"location":"airflow-helper/docs/airflow_setup/","title":"Development Container vs Docker","text":"<p>I have found that the best 2 ways to run/setup an airflow instance are with Development Containers (Dev Containers) or Docker. With the docker solution offering more flexibility and control and the Dev Containers solution offering simplicity.</p> <p>I have found the docker solution to be more useful, mostly because I have paired it with justfile commands to make the process simple and with lazydocker I can fully understand how my instance is running and all the errors associated with it. However, the dev container method does give you a clean repo and is also easy to maintain. The choice is personal preference. This repo contains code for both solutions and I have documented the process below.</p> <p>I have also written an article on the topic here</p>"},{"location":"airflow-helper/docs/airflow_setup/#running-airflow-with-a-development-container","title":"Running airflow with a Development Container","text":"<p>The project contains a configuration files for Visual Studio Code Development Containers in the <code>.devcontainer/</code> directory. To run the project with this locally, follow the below steps:</p> <ol> <li>With the project open in VSCode open the folder in the development container by clicking the prompt in the bottom right corner or running \"Reopen in Container\" command via Command Palette (usually accessible with Ctrl+Shift+P).</li> <li>First time starting the containers can be slow due to building the docker images, after it has opened validate it is working by navigating to localhost:8080 in your browser to find the airflow log in page (default username and password is 'airflow').</li> </ol>"},{"location":"airflow-helper/docs/airflow_setup/#customising-the-development-container","title":"Customising the Development Container","text":"<p>Common changes you may wish to make:</p> <ul> <li>Any additional pip requirements you have can be added to <code>requirements.txt</code> or <code>requirements-dev.txt</code>.</li> <li>In the <code>.devcontainer/docker-compose.yml</code> file:</li> <li>Set <code>PYTHON_VARIANT</code> variable to the version of python you wish to use e.g. '3.10'</li> <li>Set <code>AIRFLOW_VERSION</code> variable to the version of airflow you wish to use e.g. '2.4.2'.</li> <li>Set <code>GOOGLE_CLOUD_PROJECT</code> variable to GCP project you are using.</li> <li>Add any additional airflow providers to <code>AIRFLOW_PROVIDERS</code>.</li> <li>In the <code>.devcontainer/.env</code> file (available after building container for first time):<ul> <li>Update <code>GCP_CREDENTIALS_LOCAL</code> to point at different location for a GCP credential file.</li> </ul> </li> </ul> <p>After making any of the above changes you will need to rebuid the container by running \"Rebuild Container\" command via Command Palette (usually accessible with Ctrl+Shift+P).</p>"},{"location":"airflow-helper/docs/airflow_setup/#running-airflow-with-docker","title":"Running Airflow with Docker","text":"<p>To run airflow using Docker, instead Run <code>local-airflow</code> to create a local airflow instance for local testing.</p> <p>Large airflow instance with redis, workers, triggerer, scheduler, postgres and webserer <pre><code>just local-airflow\n</code></pre> small airflow instance with postgres and webserver <pre><code>just local-airflow-lite\n</code></pre></p> <p>To shutdown the airflow instance run <pre><code>just destroy-airflow\n</code></pre> or <pre><code>just destroy-airflow-lite\n</code></pre></p> <p>The just file also has extra commands for docker &amp; installation <pre><code># Docker commands\njust clean-docker-containers\njust force-remove-docker-containers\njust show-running-containers\njust show-all-containers\n\n# Install requirements.txt on Virtual Environment\njust install-libraries\njust uninstall-libraries\n</code></pre></p>"},{"location":"airflow-helper/docs/airflow_setup/#automated-testing","title":"Automated testing","text":"<ul> <li>In <code>docs/automation/data_dictionary</code> Edit line 56 to link to production datasets for data dictionary creation</li> <li> <p>In justfile update-unit-test-docs section &amp; update-python-docs add any folders or python scripts that you would like to be documented by lazydocs</p> </li> <li> <p>Run <code>just update-docs</code> to update all docs (python, unit tests, data dictonaries)</p> </li> <li> <p>Specific tests can be run using <code>just update-python-docs</code> <code>update-unit-test-docs</code> <code>update-data-dictionary-docs</code></p> </li> </ul>"},{"location":"airflow-helper/docs/overview/","title":"Airflow Helper","text":"<p>Airflow Repository</p> <p>This is a starter code template, with helper code. The code here is to help you set up you own repo, not to be used as a repo itself. This code is to help you get started with initialising a local aiflow instance. A local airflow instance is useful for testing and debugging your airflow code. However, it should not be used for production deployments.</p> <p>If you are using windows you will need to first install WSL2 and get your WSL instance setup.</p>"},{"location":"airflow-helper/docs/overview/#initial-wsl-and-project-setup","title":"Initial WSL and project setup","text":"<ol> <li> <p>Make sure you have a working WSL terminal (WSL2).</p> <ul> <li>Gitbash installed https://git-scm.com/downloads</li> </ul> </li> <li> <p>Clone the repository to your WSL terminal.</p> </li> <li> <p>Install just. Just is built using the Rust programming language (just git repo). There are installation instructions in the repo. We have found the most success using <code>cargo</code>. You will also need to run <code>sudo apt install build-essential</code> this is to ensure you have a C linker installed. To install cargo you can use rustup to install the rust toolchain (which cargo is a part of). Once cargo is installed (ensure you follow the last command for setup <code>. \"$HOME/.cargo/env\"</code>) you can run <code>cargo install just</code></p> </li> <li> <p>Either run <code>just project-setup</code> or <code>just full-project-setup</code>. </p> </li> </ol> <p>If this is your first time on WSL, <code>full-project-setup</code> will: </p> <ul> <li>Install all the necessary packages for package manager</li> <li>Install Python3</li> <li>configure Application Default Credentials</li> <li>Initialise SDK environment</li> <li>Install Docker</li> <li>Install Lazydocker</li> <li>Setup virtual environment and install libraries from requirements-dev.txt</li> <li>Create .env file</li> </ul> <p><code>project-setup</code> will do the project specific installation:</p> <ul> <li>Setup virtual environment and install libraries from requirements-dev.txt</li> <li>Install project specific python package</li> <li>Create .env file</li> </ul> <p>You are now ready to spin up an airflow instance \ud83d\ude80 see airflow_setup for more details</p>"},{"location":"airflow-helper/docs/the_dag_code/","title":"Useful Code for Dag Runs","text":""},{"location":"airflow-helper/docs/the_dag_code/#settings-configuration","title":"Settings Configuration","text":"<p>Configuration structures for Airflow pipeline.</p>"},{"location":"airflow-helper/docs/the_dag_code/#config.settings.PipelineConfiguration","title":"<code>PipelineConfiguration</code>  <code>dataclass</code>","text":"<p>Configuration for Airflow pipeline.</p> <p>Attributes:</p> Name Type Description <code>gcp_project</code> <code>str</code> <p>GCP project that pipeline runs in.</p> <code>bigquery_dataset</code> <code>str</code> <p>BigQuery dataset used by project.</p> <code>bucket</code> <code>str</code> <p>Bucket used by project.</p> <code>dataform_repository</code> <code>str</code> <p>Dataform repository to use.</p> <code>dataform_region</code> <code>str</code> <p>Region dataform repository sits in.</p> <code>dataform_branch</code> <code>str</code> <p>Branch of dataform repository to run.</p> <code>generic_account_username</code> <code>str</code> <p>Generic account username.</p> <code>generic_account_password_variable_name</code> <code>str</code> <p>Variable containing the password for the account.</p> Source code in <code>docs/airflow-helper/config/settings.py</code> <pre><code>@dataclass\nclass PipelineConfiguration:\n    \"\"\"Configuration for Airflow pipeline.\n\n    Attributes:\n        gcp_project (str):\n            GCP project that pipeline runs in.\n        bigquery_dataset (str):\n            BigQuery dataset used by project.\n        bucket (str):\n            Bucket used by project.\n        dataform_repository (str):\n            Dataform repository to use.\n        dataform_region (str):\n            Region dataform repository sits in.\n        dataform_branch (str):\n            Branch of dataform repository to run.\n\n        generic_account_username (str):\n            Generic account username.\n        generic_account_password_variable_name (str):\n            Variable containing the password for the account.\n    \"\"\"\n\n    gcp_project: str\n    bigquery_dataset: str\n    bucket: str\n    dataform_repository: str\n    dataform_region: str\n    dataform_branch: str\n</code></pre>"},{"location":"airflow-helper/docs/the_dag_code/#config.settings.get_pipeline_config","title":"<code>get_pipeline_config()</code>","text":"<p>Get pipeline configuration for current environment.</p> <p>Determines which pipeline configuration to used based on APP_ENV environment variable.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching pipeline for APP_ENV.</p> <p>Returns:</p> Name Type Description <code>PipelineConfiguration</code> <code>PipelineConfiguration</code> <p>Configuration for current environment.</p> Source code in <code>docs/airflow-helper/config/settings.py</code> <pre><code>def get_pipeline_config() -&gt; PipelineConfiguration:\n    \"\"\"Get pipeline configuration for current environment.\n\n    Determines which pipeline configuration to used based on\n    APP_ENV environment variable.\n\n\n    Raises:\n        ValueError: If no matching pipeline for APP_ENV.\n\n    Returns:\n        PipelineConfiguration: Configuration for current environment.\n    \"\"\"\n    app_env = os.getenv(\"APP_ENV\", \"development\")\n\n    if app_env == \"development\":\n        return development\n    if app_env == \"production\":\n        return production\n\n    raise ValueError(f\"Pipeline configuration not found for APP_ENV: {app_env}\")\n</code></pre>"},{"location":"airflow-helper/docs/the_dag_code/#tasks","title":"Tasks","text":""},{"location":"airflow-helper/docs/the_dag_code/#example_dag--example_dag","title":"example_dag","text":"<p>This an example dag used for getting started.</p>"},{"location":"airflow-helper/docs/the_dag_code/#example_dag.example_task","title":"<code>example_task()</code>","text":"<p>An example task.</p> Source code in <code>docs/airflow-helper/example_dag.py</code> <pre><code>@task()\ndef example_task():\n    \"\"\"An example task.\"\"\"\n    pass\n</code></pre>"},{"location":"airflow-helper/docs/the_dag_code/#example_dag.run_dataform","title":"<code>run_dataform()</code>","text":"<p>Compile and run the dataform pipeline.</p> Source code in <code>docs/airflow-helper/example_dag.py</code> <pre><code>@task_group()\ndef run_dataform():\n    \"\"\"Compile and run the dataform pipeline.\"\"\"\n    create_compilation_result = DataformCreateCompilationResultOperator(\n        task_id=\"create_compilation_result\",\n        retries=2,\n        project_id=settings.pipeline.gcp_project,\n        region=settings.pipeline.dataform_region,\n        repository_id=settings.pipeline.dataform_repository,\n        compilation_result={\n            \"git_commitish\": settings.pipeline.dataform_branch,\n        },\n    )\n\n    create_workflow_invocation = DataformCreateWorkflowInvocationOperator(\n        task_id=\"create_workflow_invocation\",\n        project_id=settings.pipeline.gcp_project,\n        region=settings.pipeline.dataform_region,\n        repository_id=settings.pipeline.dataform_repository,\n        workflow_invocation={\n            \"compilation_result\": \"{{ ti.xcom_pull('%s')['name'] }}\" # noqa: UP031\n            % (create_compilation_result.task_id)\n        },\n    )\n\n    create_compilation_result &gt;&gt; create_workflow_invocation\n</code></pre>"},{"location":"airflow-helper/docs/automation/data_dictionary/","title":"Automating Data Dictionary creation and maintenance","text":"<p>Nobody wants to spend all their time writing documentation, so lets make some code to do it for you. I have written a script which takes tables from any bigquery dataset (that you have access to) and generates a data dictionary for each table in the dataset within a single md file. If you display this in a centralised site such as Mkdocs you can then split your data dictionaries by dataset and they are all easily searchable through the search bar on Mkdocs. This makes looking at data very powerful as its easy to search for certain key words over many different projects.</p> <p>Please see my previous article for more information on setting up a centralised documentation site.</p>"},{"location":"airflow-helper/docs/automation/data_dictionary/#how-it-works","title":"How it works","text":"<p>This is specifically for extracting data information out of bigquery and displaying it as a data dictionary within a central documentation site.</p> <ol> <li>You need to ensure your columns descriptions are labelled appropriately. See here for information on how to do this within dataform: Centralising &amp; Automating Dataform Column Descriptions.</li> <li>You need to have access to the appropriate tables and access to the metadata tables: \"INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\"</li> <li>If your table schemas or descriptions change you will need to rerun this code and push to the repo.</li> <li>Have pandas &amp; google-cloud-bigquery installed to use the libraries</li> </ol>"},{"location":"airflow-helper/docs/automation/data_dictionary/#the-code","title":"The Code","text":""},{"location":"airflow-helper/docs/automation/data_dictionary/#automation.data_dictionary.bq_to_df","title":"<code>bq_to_df(project, dataset, table, columns)</code>","text":"<p>Extracts table from BigQuery and returns a dataframe of the table</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>project location of table</p> required <code>dataset</code> <code>str</code> <p>dataset location of table</p> required <code>table</code> <code>str</code> <p>table name of table</p> required <code>columns</code> <code>str</code> <p>columns to be extracted from table (split by comma)</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>dataframe</code> <p>dataframe of data within table for specified columns</p> Source code in <code>docs/airflow-helper/automation/data_dictionary.py</code> <pre><code>def bq_to_df(project, dataset, table, columns):\n    \"\"\"\n    Extracts table from BigQuery and returns a dataframe of the table\n\n    Args:\n        project (str): project location of table\n        dataset (str): dataset location of table\n        table (str): table name of table\n        columns (str): columns to be extracted from table (split by comma)\n\n    Returns:\n        df (dataframe): dataframe of data within table for specified columns\n    \"\"\"\n    client = bigquery.Client()\n    sql = f\"SELECT {columns} FROM `{project}.{dataset}.{table}`\"\n    df = client.query(sql).to_dataframe()\n    return df\n</code></pre>"},{"location":"airflow-helper/docs/automation/data_dictionary/#automation.data_dictionary.create_md_file_from_string","title":"<code>create_md_file_from_string(string, filename)</code>","text":"<p>creates or opens a file based on a file name and writes/overwrites the data within the file with from the string input</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>string to be written into the file</p> required <code>filename</code> <code>str</code> <p>name and location of file to be written to</p> required Source code in <code>docs/airflow-helper/automation/data_dictionary.py</code> <pre><code>def create_md_file_from_string(string, filename):\n    \"\"\"\n    creates or opens a file based on a file name and writes/overwrites the data within the file with from the string input\n\n    Args:\n        string (str): string to be written into the file\n        filename (str): name and location of file to be written to\n    \"\"\"\n    with open(filename, \"w\") as file:\n        file.write(string)\n</code></pre>"},{"location":"airflow-helper/docs/automation/data_dictionary/#automation.data_dictionary.extract_information_schema_to_markdown","title":"<code>extract_information_schema_to_markdown(project, dataset)</code>","text":"<p>Extract the information schema from BQ and translate to markdown</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>project location of table</p> required <code>dataset</code> <code>str</code> <p>dataset location of table</p> required Source code in <code>docs/airflow-helper/automation/data_dictionary.py</code> <pre><code>def extract_information_schema_to_markdown(project, dataset):\n    \"\"\"\n    Extract the information schema from BQ and translate to markdown\n\n    Args:\n        project (str): project location of table\n        dataset (str): dataset location of table\n    \"\"\"\n    columns = \"table_name, column_name, data_type, description, collation_name, rounding_mode\"\n    table = \"INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\"\n\n    df = bq_to_df(project, dataset, table, columns)\n    tables = df[\"table_name\"].unique()\n    md_string = f\"# {dataset} \\n \\n## Data Dictionary for {dataset} in {project} \\n\"\n    for t in tables:\n        filtered_df = df.loc[df[\"table_name\"] == t]\n        filtered_df = filtered_df.reset_index(drop=True)\n        table_string = filtered_df.to_markdown()\n        md_string = md_string + \"\\n\" + f\"### {t}\" + \"\\n\" + table_string + \"\\n\"\n    create_md_file_from_string(md_string, f\"docs/data_dictionary/{dataset}.md\")\n</code></pre>"},{"location":"bq-dataform-helper/","title":"dataform-helper","text":"<p>This is a starter code template with some useful snippets of code to help you set up your own dataform repo. This should not be used as a direct template itself but as a guide to understanding the basics of a simple dataform project setup.</p>"},{"location":"bq-dataform-helper/#folder-structure","title":"Folder Structure","text":"<p>The repo has a simple structure, with a definitions folder and a includes folder. The defitions folder should contain all of your sql code and I would advise you to split this up into sections for when your project grows bigger (i.e views, sources, etc). The includes folder contains any scripts needed for dataform to operate, this could include bespoke js scripts/tests for your tables.</p> <p>The dataform.json file determines the default settings for your project and unless overwritten at runtime, will be the default settings for all dataform pipeline runs.</p> <p>package.json defines the dataform version being used by the pipeline</p>"},{"location":"bq-dataform-helper/#sqlx-files","title":"SQLX files","text":"<p>Browse the repo to see how the setup is for different files. See the example below for standard setup:</p> <pre><code>config {\n    type: \"table\",\n    tags: [\"tag\"],\n    assertions: {\n        uniqueKey: [\"year\", \"day\", \"key\"],\n        nonNull: [\"key\"],\n        rowConditions: [\n            \"REGEXP_CONTAINS(year, r'^[0-9]{4}')\"\n        ],\n    },\n    description: \"This table contains exciting data\",\n    columns: constants.getSchemaDescriptions({})\n}\n\nSELECT \n  year,\n  day,\n  key\nFROM ${ref('table_1')}\n</code></pre>"},{"location":"bq-dataform-helper/#centralising-automating-dataform-column-descriptions","title":"Centralising &amp; Automating Dataform Column Descriptions","text":"<p>This is a quick guide which shows you how to centralise your column descriptions so they can be updated/edited in one place. The advantage of this is that you can avoid duplication in column descriptions and ensure the column descriptions across your dataset are identical and do not diverge. Nobody wants to go into a repo and have to edit every tables schema when a change is made.</p> <p>click here for more details</p>"},{"location":"bq-dataform-helper/docs/dataform_column_descriptions/","title":"Centralising &amp; Automating Dataform Column Descriptions","text":"<p>This is a quick guide which shows you how to centralise your column descriptions so they can be updated/edited in one place. The advantage of this is that you can avoid duplication in column descriptions and ensure the column descriptions across your dataset are identical and do not diverge. Nobody wants to go into a repo and have to edit every tables schema when a change is made.</p>"},{"location":"bq-dataform-helper/docs/dataform_column_descriptions/#how-it-works","title":"How it works","text":"<p>The backbone of dataform is javascript, the benefit of this is that they allow you to use javascript code within dataform. Here is a good article showcasing some of the features with javascript within dataform: https://blog.datatovalue.nl/dataform-javascript-utility-functions-for-ga4-tables-257dd54b034e</p> <p>The code to generate the schema descriptions is shown below:</p> <pre><code>const schemaDescriptions = {   \n    /*\n     * Column descriptions\n     */\ncolumn_one: \"Column One description, succinct and to the point\",\ncolumn_two: \"Column two description\" \n}\n\nconst getSchemaDescriptions = (customDescriptions) =&gt; {\n  return Object.assign({}, schemaDescriptions, customDescriptions);\n}\n\nmodule.exports = { getSchemaDescriptions };\n</code></pre> <p>Put this code in a constants.js file located in the includes folder in Dataform. This folder allows you to reuse code across your repository.</p> <p></p>"},{"location":"bq-dataform-helper/docs/dataform_column_descriptions/#applying-the-schema-code","title":"Applying the schema code","text":"<p>Once you have this javascript code you can implement it in all your SQLX scripts. Apply the function to columns in the config to ensure the descriptions are added to the table.</p> <pre><code>config {\n  type: \"table\",\n  columns: constants.getSchemaDescriptions()\n}\n\nSELECT\n  column_one\n  column_two\nFROM ${ref(\"table\")}\n</code></pre> <p>When you have run your dataform pipeline your table will be created with the column descriptions added as shown below.</p> <p></p> <p>Its as simple as that! No more excuses for missing column descriptions.</p>"},{"location":"bq-dataform-helper/docs/overview/","title":"Dataform Helper","text":"<p>Dataform Repository</p> <p>This is a starter code template with some useful snippets of code to help you set up your own dataform repo. This should not be used as a direct template itself but as a guide to understanding the basics of a simple dataform project setup.</p>"},{"location":"bq-dataform-helper/docs/overview/#folder-structure","title":"Folder Structure","text":"<p>The repo has a simple structure, with a definitions folder and a includes folder. The defitions folder should contain all of your sql code and I would advise you to split this up into sections for when your project grows bigger (i.e views, sources, etc). The includes folder contains any scripts needed for dataform to operate, this could include bespoke js scripts/tests for your tables.</p> <p>The dataform.json file determines the default settings for your project and unless overwritten at runtime, will be the default settings for all dataform pipeline runs.</p> <p>package.json defines the dataform version being used by the pipeline</p>"},{"location":"bq-dataform-helper/docs/overview/#sqlx-files","title":"SQLX files","text":"<p>Browse the repo to see how the setup is for different files. See the example below for standard setup:</p> <pre><code>config {\n    type: \"table\",\n    tags: [\"tag\"],\n    assertions: {\n        uniqueKey: [\"year\", \"day\", \"key\"],\n        nonNull: [\"key\"],\n        rowConditions: [\n            \"REGEXP_CONTAINS(year, r'^[0-9]{4}')\"\n        ],\n    },\n    description: \"This table contains exciting data\",\n    columns: constants.getSchemaDescriptions({})\n}\n\nSELECT \n  year,\n  day,\n  key\nFROM ${ref('table_1')}\n</code></pre>"},{"location":"bq-dataform-helper/docs/overview/#centralising-automating-dataform-column-descriptions","title":"Centralising &amp; Automating Dataform Column Descriptions","text":"<p>This is a quick guide which shows you how to centralise your column descriptions so they can be updated/edited in one place. The advantage of this is that you can avoid duplication in column descriptions and ensure the column descriptions across your dataset are identical and do not diverge. Nobody wants to go into a repo and have to edit every tables schema when a change is made.</p> <p>click here for more details</p>"},{"location":"dataform-helper/dataform_column_descriptions/","title":"Centralising &amp; Automating Dataform Column Descriptions","text":"<p>This is a quick guide which shows you how to centralise your column descriptions so they can be updated/edited in one place. The advantage of this is that you can avoid duplication in column descriptions and ensure the column descriptions across your dataset are identical and do not diverge. Nobody wants to go into a repo and have to edit every tables schema when a change is made.</p>"},{"location":"dataform-helper/dataform_column_descriptions/#how-it-works","title":"How it works","text":"<p>The backbone of dataform is javascript, the benefit of this is that they allow you to use javascript code within dataform. Here is a good article showcasing some of the features with javascript within dataform: https://blog.datatovalue.nl/dataform-javascript-utility-functions-for-ga4-tables-257dd54b034e</p> <p>The code to generate the schema descriptions is shown below:</p> <pre><code>const schemaDescriptions = {   \n    /*\n     * Column descriptions\n     */\ncolumn_one: \"Column One description, succinct and to the point\",\ncolumn_two: \"Column two description\" \n}\n\nconst getSchemaDescriptions = (customDescriptions) =&gt; {\n  return Object.assign({}, schemaDescriptions, customDescriptions);\n}\n\nmodule.exports = { getSchemaDescriptions };\n</code></pre> <p>Put this code in a constants.js file located in the includes folder in Dataform. This folder allows you to reuse code across your repository.</p> <p></p>"},{"location":"dataform-helper/dataform_column_descriptions/#applying-the-schema-code","title":"Applying the schema code","text":"<p>Once you have this javascript code you can implement it in all your SQLX scripts. Apply the function to columns in the config to ensure the descriptions are added to the table.</p> <pre><code>config {\n  type: \"table\",\n  columns: constants.getSchemaDescriptions()\n}\n\nSELECT\n  column_one\n  column_two\nFROM ${ref(\"table\")}\n</code></pre> <p>When you have run your dataform pipeline your table will be created with the column descriptions added as shown below.</p> <p></p> <p>Its as simple as that! No more excuses for missing column descriptions.</p>"},{"location":"dataform-helper/overview/","title":"Dataform Helper","text":"<p>Dataform Repository</p> <p>This is a starter code template with some useful snippets of code to help you set up your own dataform repo. This should not be used as a direct template itself but as a guide to understanding the basics of a simple dataform project setup.</p>"},{"location":"dataform-helper/overview/#folder-structure","title":"Folder Structure","text":"<p>The repo has a simple structure, with a definitions folder and a includes folder. The defitions folder should contain all of your sql code and I would advise you to split this up into sections for when your project grows bigger (i.e views, sources, etc). The includes folder contains any scripts needed for dataform to operate, this could include bespoke js scripts/tests for your tables.</p> <p>The dataform.json file determines the default settings for your project and unless overwritten at runtime, will be the default settings for all dataform pipeline runs.</p> <p>package.json defines the dataform version being used by the pipeline</p>"},{"location":"dataform-helper/overview/#sqlx-files","title":"SQLX files","text":"<p>Browse the repo to see how the setup is for different files. See the example below for standard setup:</p> <pre><code>config {\n    type: \"table\",\n    tags: [\"tag\"],\n    assertions: {\n        uniqueKey: [\"year\", \"day\", \"key\"],\n        nonNull: [\"key\"],\n        rowConditions: [\n            \"REGEXP_CONTAINS(year, r'^[0-9]{4}')\"\n        ],\n    },\n    description: \"This table contains exciting data\",\n    columns: constants.getSchemaDescriptions({})\n}\n\nSELECT \n  year,\n  day,\n  key\nFROM ${ref('table_1')}\n</code></pre>"},{"location":"dataform-helper/overview/#centralising-automating-dataform-column-descriptions","title":"Centralising &amp; Automating Dataform Column Descriptions","text":"<p>This is a quick guide which shows you how to centralise your column descriptions so they can be updated/edited in one place. The advantage of this is that you can avoid duplication in column descriptions and ensure the column descriptions across your dataset are identical and do not diverge. Nobody wants to go into a repo and have to edit every tables schema when a change is made.</p> <p>click here for more details</p>"},{"location":"streamlit-helper/","title":"streamlit-helper","text":"<p>Example Repo for some of the basics of streamlit to get you started with a working repo running locally and in the cloud (GCP). This repo gives you a cheat sheet and examples of the functionality within streamlit including</p> <ul> <li>Buttons</li> <li>Messages</li> <li>File upload</li> <li>Tables</li> <li>Graphs</li> <li>Forms</li> <li>Chat</li> <li>Camera</li> </ul> <p>Use the template code to get started and try the functionality of streamlit from your own machine.</p>"},{"location":"streamlit-helper/#getting-started-locally","title":"Getting started Locally","text":"<ol> <li>Clone this repo</li> <li>Install libraries <code>pip install -r requirements.txt</code> - ideally to your own virtual environment</li> <li>Run <code>streamlit run main.py</code></li> <li>Its as easy as that \ud83d\udc68\u200d\ud83d\udcbb</li> </ol>"},{"location":"streamlit-helper/#deploying-to-gcp","title":"Deploying to GCP","text":"<p>Deploying your streamlit app to gcp can be simple if you know what you are doing... Streamlit has some small issues (related to websockets) when it comes to deplying to app engine, this can can make it slightly more tricky.</p> <p>This article here written by Yuichiro Tachibana (Tsuchiya) explains the best way to deploy to app engine. However, if you have organisation policies that block external IP Access you will not be able to deploy a flexible environment. This will mean you will need to use Cloud Run.</p> <p>Whether you use Cloud Run or App Engine it is best to use IAP (Identity Aware Proxy) to secure your app</p> <p>Deploying to cloud Run: Link</p> <p>Deploying to App Engine: Link</p>"},{"location":"streamlit-helper/deploying_to_app_engine/","title":"Deploying to App Engine","text":""},{"location":"streamlit-helper/deploying_to_cloud_run/","title":"Deploying to Cloud Run","text":""},{"location":"streamlit-helper/overview/","title":"Streamlit Helper","text":"<p>Streamlit Repository</p> <p>Example Repo for some of the basics of streamlit to get you started with a working repo running locally and in the cloud (GCP). This repo gives you a cheat sheet and examples of the functionality within streamlit including</p> <ul> <li>Buttons</li> <li>Messages</li> <li>File upload</li> <li>Tables</li> <li>Graphs</li> <li>Forms</li> <li>Chat</li> <li>Camera</li> </ul> <p>Use the template code to get started and try the functionality of streamlit from your own machine.</p>"},{"location":"streamlit-helper/overview/#getting-started-locally","title":"Getting started Locally","text":"<ol> <li>Clone this repo</li> <li>Install libraries <code>pip install -r requirements.txt</code> - ideally to your own virtual environment</li> <li>Run <code>streamlit run main.py</code></li> <li>Its as easy as that \ud83d\udc68\u200d\ud83d\udcbb</li> </ol>"},{"location":"streamlit-helper/overview/#deploying-to-gcp","title":"Deploying to GCP","text":"<p>Deploying your streamlit app to gcp can be simple if you know what you are doing... Streamlit has some small issues (related to websockets) when it comes to deplying to app engine, this can can make it slightly more tricky.</p> <p>This article here written by Yuichiro Tachibana (Tsuchiya) explains the best way to deploy to app engine. However, if you have organisation policies that block external IP Access you will not be able to deploy a flexible environment. This will mean you will need to use Cloud Run.</p> <p>Whether you use Cloud Run or App Engine it is best to use IAP (Identity Aware Proxy) to secure your app</p> <p>Deploying to cloud Run</p> <p>Deploying to App Engine</p>"},{"location":"streamlit-helper/setting_up_auth/","title":"Setting Up Authentication with Identity Aware Proxy (IAP)","text":""},{"location":"streamlit-helper/docs/deploying_to_app_engine/","title":"Deploying to App Engine","text":""},{"location":"streamlit-helper/docs/deploying_to_cloud_run/","title":"Deploying to Cloud Run","text":""},{"location":"streamlit-helper/docs/overview/","title":"Streamlit Helper","text":"<p>Streamlit Repository</p> <p>Example Repo for some of the basics of streamlit to get you started with a working repo running locally and in the cloud (GCP). This repo gives you a cheat sheet and examples of the functionality within streamlit including</p> <ul> <li>Buttons</li> <li>Messages</li> <li>File upload</li> <li>Tables</li> <li>Graphs</li> <li>Forms</li> <li>Chat</li> <li>Camera</li> </ul> <p>Use the template code to get started and try the functionality of streamlit from your own machine.</p>"},{"location":"streamlit-helper/docs/overview/#getting-started-locally","title":"Getting started Locally","text":"<ol> <li>Clone this repo</li> <li>Install libraries <code>pip install -r requirements.txt</code> - ideally to your own virtual environment</li> <li>Run <code>streamlit run main.py</code></li> <li>Its as easy as that \ud83d\udc68\u200d\ud83d\udcbb</li> </ol>"},{"location":"streamlit-helper/docs/overview/#deploying-to-gcp","title":"Deploying to GCP","text":"<p>Deploying your streamlit app to gcp can be simple if you know what you are doing... Streamlit has some small issues (related to websockets) when it comes to deplying to app engine, this can can make it slightly more tricky.</p> <p>This article here written by Yuichiro Tachibana (Tsuchiya) explains the best way to deploy to app engine. However, if you have organisation policies that block external IP Access you will not be able to deploy a flexible environment. This will mean you will need to use Cloud Run.</p> <p>Whether you use Cloud Run or App Engine it is best to use IAP (Identity Aware Proxy) to secure your app</p> <p>Deploying to cloud Run</p> <p>Deploying to App Engine</p>"},{"location":"streamlit-helper/docs/setting_up_auth/","title":"Setting Up Authentication with Identity Aware Proxy (IAP)","text":""}]}